# üéØ Multi-Modal Coordinator Service - Core Engine Implementation
# AURA AI √áok Modlu Sorgu Desteƒüi Sistemi

"""
Multi-Modal Query Coordinator - Ana Engine

Bu mod√ºl AURA AI sisteminin √ßok modlu sorgu i≈üleme yeteneklerini saƒülar.
Kullanƒ±cƒ±lardan gelen g√∂rsel ve metin verilerini entegre ederek,
akƒ±llƒ± moda √∂nerileri √ºretir.

√ñzellikler:
- CLIP tabanlƒ± g√∂rsel analiz
- NLU tabanlƒ± metin analizi  
- Context fusion ve semantic integration
- Multi-service coordination
- Real-time response generation
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import base64
import io
from PIL import Image
import numpy as np

# FastAPI ve Pydantic imports
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from pydantic import BaseModel, Field
import httpx

# Logging configuration - Her satƒ±rda a√ßƒ±klama yaparak takip edilebilirlik saƒülƒ±yoruz
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("multi_modal_coordinator")

class QueryType(Enum):
    """√áok modlu sorgu tiplerini tanƒ±mlayan enum sƒ±nƒ±fƒ±"""
    SHIRT_COMBINATION = "shirt_combination"          # G√∂mlek kombin sorgularƒ±
    DRESS_SHOE_MATCHING = "dress_shoe_matching"      # Elbise ayakkabƒ± uyumu
    PANTS_JACKET_PAIRING = "pants_jacket_pairing"    # Pantolon ceket kombinasyonu
    BAG_OUTFIT_STYLING = "bag_outfit_styling"        # √áanta merkezli styling
    GENERAL_STYLING = "general_styling"              # Genel styling sorgularƒ±

class FormalityLevel(Enum):
    """Giyim par√ßalarƒ±nƒ±n formallik d√ºzeylerini tanƒ±mlayan enum"""
    VERY_CASUAL = "very_casual"      # √áok rahat (spor, ev)
    CASUAL = "casual"                # G√ºnl√ºk (arkada≈ü bulu≈ümasƒ±, alƒ±≈üveri≈ü)
    SMART_CASUAL = "smart_casual"    # ≈ûƒ±k g√ºnl√ºk (i≈ü yemeƒüi, sosyal etkinlik)
    BUSINESS = "business"            # ƒ∞≈ü (ofis, toplantƒ±)
    FORMAL = "formal"                # Resmi (√∂zel davet, tiyatro)
    BLACK_TIE = "black_tie"          # √áok resmi (gala, kokteyl)

@dataclass
class ImageAnalysisResult:
    """G√∂rsel analiz sonu√ßlarƒ±nƒ± tutan veri sƒ±nƒ±fƒ±"""
    item_type: str                    # Giyim par√ßasƒ± tipi (g√∂mlek, pantolon vb.)
    color_palette: List[str]          # Renk paleti listesi
    dominant_color: str               # Baskƒ±n renk
    style_category: str               # Stil kategorisi (casual, formal vb.)
    formality_level: FormalityLevel   # Formallik d√ºzeyi
    fabric_type: Optional[str]        # Kuma≈ü tipi tahmini
    pattern_type: str                 # Desen tipi (d√ºz, √ßizgili, desenli)
    confidence_score: float           # Analiz g√ºven skoru (0-1)
    detailed_features: Dict[str, Any] # Detaylƒ± √∂zellikler s√∂zl√ºƒü√º
    processing_time_ms: float        # ƒ∞≈ülem s√ºresi (milisaniye)

@dataclass  
class TextAnalysisResult:
    """Metin analiz sonu√ßlarƒ±nƒ± tutan veri sƒ±nƒ±fƒ±"""
    intent: str                       # Kullanƒ±cƒ± niyeti (kombin_isteme, uyum_sorgulama vb.)
    entities: List[str]               # √áƒ±karƒ±lan varlƒ±klar (g√∂mlek, ayakkabƒ± vb.)
    context_markers: List[str]        # Baƒülam belirte√ßleri (renk, stil, ocasyon)
    preference_indicators: List[str]  # Tercih g√∂stergeleri
    query_type: QueryType            # Sorgu tipi kategorizasyonu
    confidence_score: float          # Analiz g√ºven skoru (0-1)
    sentiment: str                    # Duygu durumu (positive, neutral, negative)
    urgency_level: str               # Aciliyet d√ºzeyi (low, medium, high)
    processing_time_ms: float        # ƒ∞≈ülem s√ºresi (milisaniye)

@dataclass
class FusedContext:
    """G√∂rsel ve metin analizlerinin birle≈ütirilmi≈ü baƒülamƒ±nƒ± tutan sƒ±nƒ±f"""
    visual_context: ImageAnalysisResult      # G√∂rsel analiz sonucu
    textual_context: TextAnalysisResult     # Metin analiz sonucu
    user_profile: Dict[str, Any]            # Kullanƒ±cƒ± profil bilgileri
    unified_intent: str                     # Birle≈ütirilmi≈ü kullanƒ±cƒ± niyeti
    recommendation_type: str                # √ñneri tipi
    fusion_confidence: float                # Birle≈ütirme g√ºven skoru
    contextual_priorities: List[str]        # Baƒülamsal √∂ncelikler
    processing_metadata: Dict[str, Any]     # ƒ∞≈üleme metadata'sƒ±

class CLIPImageProcessor:
    """CLIP modeli tabanlƒ± g√∂rsel analiz i≈ülemcisi"""
    
    def __init__(self):
        """CLIPImageProcessor sƒ±nƒ±fƒ±nƒ± ba≈ülatƒ±r ve gerekli model y√ºklemelerini yapar"""
        logger.info("üé® CLIP Image Processor initializing...")
        # Ger√ßek implementasyonda burada CLIP model y√ºkleme i≈ülemi olacak
        # ≈ûimdilik mock implementation kullanƒ±yoruz
        self.model_loaded = True
        self.supported_formats = ['JPEG', 'PNG', 'JPG', 'WEBP']
        logger.info("‚úÖ CLIP Image Processor initialized successfully")
    
    async def analyze_image(self, image_data: bytes, query_context: str = "") -> ImageAnalysisResult:
        """
        G√∂rsel analizi ger√ßekle≈ütiren ana metod
        
        Args:
            image_data: G√∂rsel verisi (bytes formatƒ±nda)
            query_context: Sorgu baƒülamƒ± (opsiyonel)
            
        Returns:
            ImageAnalysisResult: Detaylƒ± g√∂rsel analiz sonucu
        """
        start_time = datetime.now()
        logger.info(f"üîç Starting image analysis with context: {query_context}")
        
        try:
            # G√∂rseli PIL Image objesine d√∂n√º≈üt√ºr
            image = Image.open(io.BytesIO(image_data))
            
            # G√∂rsel format kontrol√º
            if image.format not in self.supported_formats:
                raise ValueError(f"Unsupported image format: {image.format}")
            
            # G√∂rsel boyut ve kalite kontrol√º
            width, height = image.size
            logger.info(f"üìê Image dimensions: {width}x{height}")
            
            # Mock CLIP analysis - Ger√ßek implementasyonda CLIP model inference olacak
            analysis_result = await self._mock_clip_analysis(image, query_context)
            
            # ƒ∞≈ülem s√ºresini hesapla
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            analysis_result.processing_time_ms = processing_time
            
            logger.info(f"‚úÖ Image analysis completed in {processing_time:.2f}ms")
            logger.info(f"üè∑Ô∏è Detected item: {analysis_result.item_type} with {analysis_result.confidence_score:.2f} confidence")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"‚ùå Image analysis failed: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Image analysis failed: {str(e)}")
    
    async def _mock_clip_analysis(self, image: Image, context: str) -> ImageAnalysisResult:
        """
        Mock CLIP analizi - Ger√ßek implementasyon i√ßin placeholder
        Ger√ßek projede burada CLIP model inference yapƒ±lacak
        """
        # Simulated processing delay - ger√ßek model inference s√ºresini sim√ºle eder
        await asyncio.sleep(0.1)
        
        # Context'e g√∂re farklƒ± sonu√ßlar d√∂ner (akƒ±llƒ± mock)
        if "g√∂mlek" in context.lower() or "shirt" in context.lower():
            return ImageAnalysisResult(
                item_type="g√∂mlek",
                color_palette=["mavi", "beyaz"],
                dominant_color="mavi",
                style_category="business_casual",
                formality_level=FormalityLevel.BUSINESS,
                fabric_type="pamuk",
                pattern_type="d√ºz",
                confidence_score=0.92,
                detailed_features={
                    "collar_type": "klasik",
                    "sleeve_length": "uzun",
                    "fit_type": "slim",
                    "button_style": "standart"
                },
                processing_time_ms=0.0  # Ger√ßek s√ºre sonradan set edilecek
            )
        elif "elbise" in context.lower() or "dress" in context.lower():
            return ImageAnalysisResult(
                item_type="elbise",
                color_palette=["siyah", "g√ºm√º≈ü"],
                dominant_color="siyah",
                style_category="formal",
                formality_level=FormalityLevel.FORMAL,
                fabric_type="polyester",
                pattern_type="d√ºz",
                confidence_score=0.89,
                detailed_features={
                    "dress_length": "midi",
                    "neckline": "V_neck",
                    "fit_type": "A_line",
                    "occasion_suitability": "evening"
                },
                processing_time_ms=0.0
            )
        else:
            # Genel analiz sonucu
            return ImageAnalysisResult(
                item_type="giyim_par√ßasƒ±",
                color_palette=["belirtilmemi≈ü"],
                dominant_color="belirtilmemi≈ü",
                style_category="genel",
                formality_level=FormalityLevel.CASUAL,
                fabric_type="belirtilmemi≈ü",
                pattern_type="belirtilmemi≈ü",
                confidence_score=0.75,
                detailed_features={},
                processing_time_ms=0.0
            )

class NLUTextProcessor:
    """Natural Language Understanding tabanlƒ± metin analiz i≈ülemcisi"""
    
    def __init__(self):
        """NLUTextProcessor sƒ±nƒ±fƒ±nƒ± ba≈ülatƒ±r ve NLU servis baƒülantƒ±sƒ±nƒ± kurar"""
        logger.info("üß† NLU Text Processor initializing...")
        self.nlu_service_url = "http://localhost:8002"  # NLU Service endpoint
        self.client = httpx.AsyncClient(timeout=10.0)
        logger.info("‚úÖ NLU Text Processor initialized successfully")
    
    async def analyze_text(self, text_query: str, image_context: str = "") -> TextAnalysisResult:
        """
        Metin analizi ger√ßekle≈ütiren ana metod
        
        Args:
            text_query: Kullanƒ±cƒ±nƒ±n metin sorgusu
            image_context: G√∂rsel baƒülam bilgisi (opsiyonel)
            
        Returns:
            TextAnalysisResult: Detaylƒ± metin analiz sonucu
        """
        start_time = datetime.now()
        logger.info(f"üìù Starting text analysis for query: '{text_query}'")
        
        try:
            # NLU servisine istek g√∂nder
            nlu_request = {
                "text": text_query,
                "context": {"image_context": image_context},
                "analysis_type": "fashion_query"
            }
            
            # Mock NLU analysis - Ger√ßek implementasyonda NLU service √ßaƒürƒ±sƒ± olacak
            analysis_result = await self._mock_nlu_analysis(text_query, image_context)
            
            # ƒ∞≈ülem s√ºresini hesapla
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            analysis_result.processing_time_ms = processing_time
            
            logger.info(f"‚úÖ Text analysis completed in {processing_time:.2f}ms")
            logger.info(f"üéØ Detected intent: {analysis_result.intent} with {analysis_result.confidence_score:.2f} confidence")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"‚ùå Text analysis failed: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Text analysis failed: {str(e)}")
    
    async def _mock_nlu_analysis(self, text: str, context: str) -> TextAnalysisResult:
        """
        Advanced NLU analysis with Prompt Engineering Patterns
        Implements: Persona, Recipe, Template, and Context & Instruction patterns
        """
        # Simulate realistic processing delay
        await asyncio.sleep(0.08)
        
        logger.info(f"üß† Advanced NLU analysis starting for: '{text[:50]}...'")
        
        # PERSONA PATTERN: Expert Style Consultant Role
        persona_context = """
        Sen AURA AI'nƒ±n uzman stil danƒ±≈ümanƒ±sƒ±n. 10 yƒ±llƒ±k moda end√ºstrisi deneyimin var.
        Ki≈üiliƒüin: Kullanƒ±cƒ±nƒ±n ki≈üisel stilini anlayan empatik danƒ±≈üman
        Uzmanlƒ±ƒüƒ±n: Trend ve klasik stil bilgisi, her v√ºcut tipine uygun √∂neriler
        """
        
        # RECIPE PATTERN: Step-by-step analysis framework
        analysis_steps = [
            "1. Sorguyu temizle ve normalize et",
            "2. Ana niyeti tespit et",
            "3. Varlƒ±klarƒ± (entities) √ßƒ±kar",
            "4. Baƒülamsal i≈üaretleri belirle",
            "5. Tercih g√∂stergelerini analiz et",
            "6. G√ºven skorunu hesapla"
        ]
        
        # TEMPLATE PATTERN: Structured intent classification
        intent_templates = {
            "combination_request": {
                "patterns": ["ne giyeyim", "kombin", "√∂ner", "uygun", "birlikte", "hangi kƒ±yafet"],
                "entities": ["kombin", "giyim", "√∂neri", "uygun", "stil"],
                "confidence_base": 0.92
            },
            "single_item_query": {
                "patterns": ["bu ... ile", "hangi", "ne giyebilirim", "uyumlu", "e≈üle≈ütir"],
                "entities": ["e≈üle≈ütirme", "uyum", "birlikte"],
                "confidence_base": 0.89
            },
            "shoe_matching": {
                "patterns": ["ayakkabƒ±", "uygun", "var mƒ±", "hangi ayakkabƒ±", "shoe"],
                "entities": ["ayakkabƒ±", "uygun", "uyum"],
                "confidence_base": 0.91
            },
            "jacket_pairing": {
                "patterns": ["ceket", "√∂ner", "hangi ceket", "jacket", "blazer"],
                "entities": ["ceket", "√∂neri", "uygun"],
                "confidence_base": 0.88
            },
            "weather_based": {
                "patterns": ["hava", "soƒüuk", "sƒ±cak", "yaƒümur", "kar", "weather"],
                "entities": ["hava", "soƒüuk", "sƒ±cak", "mevsim"],
                "confidence_base": 0.94
            },
            "occasion_specific": {
                "patterns": ["ofis", "toplantƒ±", "parti", "yemek", "bulu≈üma", "work", "meeting"],
                "entities": ["etkinlik", "durum", "ortam"],
                "confidence_base": 0.87
            }
        }
        
        # CONTEXT & INSTRUCTION PATTERN: Rich contextual analysis
        text_lower = text.lower()
        
        # Advanced pattern matching with confidence scoring
        detected_intent = "general_styling"
        detected_entities = []
        context_markers = []
        preference_indicators = []
        confidence = 0.70
        query_type = QueryType.GENERAL_STYLING
        
        # Weather-based outfit query detection
        if any(pattern in text_lower for pattern in intent_templates["weather_based"]["patterns"]):
            detected_intent = "hava_durumu_bazli_outfit"
            detected_entities = ["hava", "giyim", "uygun"]
            context_markers = ["mevsimsel", "praktik", "koruma"]
            preference_indicators = ["uygun", "rahat", "fonksiyonel"]
            confidence = intent_templates["weather_based"]["confidence_base"]
            query_type = QueryType.GENERAL_STYLING
            
        # Shoe matching query detection
        elif any(pattern in text_lower for pattern in intent_templates["shoe_matching"]["patterns"]):
            detected_intent = "ayakkabƒ±_uyumu_sorgulama"
            detected_entities = ["ayakkabƒ±", "uygun", "elbise"]
            context_markers = ["renk", "stil", "uyum", "formallƒ±k"]
            preference_indicators = ["uygun", "uyumlu", "yakƒ±≈üƒ±r"]
            confidence = intent_templates["shoe_matching"]["confidence_base"]
            query_type = QueryType.DRESS_SHOE_MATCHING
            
        # Jacket pairing query detection
        elif any(pattern in text_lower for pattern in intent_templates["jacket_pairing"]["patterns"]):
            detected_intent = "ceket_√∂nerisi_isteme"
            detected_entities = ["ceket", "pantolon", "√∂neri"]
            context_markers = ["stil", "uyum", "kombin", "profesyonel"]
            preference_indicators = ["√∂neri", "uygun", "yakƒ±≈üƒ±r"]
            confidence = intent_templates["jacket_pairing"]["confidence_base"]
            query_type = QueryType.PANTS_JACKET_PAIRING
            
        # Single item pairing query detection
        elif any(pattern in text_lower for pattern in intent_templates["single_item_query"]["patterns"]):
            detected_intent = "tek_par√ßa_e≈üle≈ütirme"
            detected_entities = ["e≈üle≈ütirme", "uyum", "kombinasyon"]
            context_markers = ["renk", "stil", "uyum"]
            preference_indicators = ["uygun", "g√ºzel", "yakƒ±≈üƒ±r"]
            confidence = intent_templates["single_item_query"]["confidence_base"]
            query_type = QueryType.SHIRT_COMBINATION
            
        # Occasion-specific query detection
        elif any(pattern in text_lower for pattern in intent_templates["occasion_specific"]["patterns"]):
            detected_intent = "etkinlik_bazli_kiyafet"
            detected_entities = ["etkinlik", "durum", "uygun"]
            context_markers = ["formallƒ±k", "ortam", "sosyal"]
            preference_indicators = ["uygun", "yakƒ±≈üƒ±r", "profesyonel"]
            confidence = intent_templates["occasion_specific"]["confidence_base"]
            query_type = QueryType.GENERAL_STYLING
            
        # General combination request
        elif any(pattern in text_lower for pattern in intent_templates["combination_request"]["patterns"]):
            detected_intent = "kombin_√∂nerisi_isteme"
            detected_entities = ["giyim", "kombin", "√∂neri"]
            context_markers = ["g√ºnl√ºk", "uygun", "stil"]
            preference_indicators = ["beƒüeni", "uyum", "g√ºzel"]
            confidence = intent_templates["combination_request"]["confidence_base"]
            query_type = QueryType.SHIRT_COMBINATION
        
        # Sentiment analysis based on language patterns
        sentiment = "neutral"
        if any(word in text_lower for word in ["m√ºkemmel", "harika", "g√ºzel", "beƒüeniyorum"]):
            sentiment = "positive"
        elif any(word in text_lower for word in ["hi√ß", "berbat", "k√∂t√º", "beƒüenmiyorum"]):
            sentiment = "negative"
        
        # Urgency level detection
        urgency = "medium"
        if any(word in text_lower for word in ["acil", "hemen", "≈üimdi", "bug√ºn"]):
            urgency = "high"
        elif any(word in text_lower for word in ["gelecekte", "sonra", "belki"]):
            urgency = "low"
        
        # Apply confidence boost for multi-pattern matches
        pattern_matches = sum(1 for template in intent_templates.values() 
                            if any(pattern in text_lower for pattern in template["patterns"]))
        if pattern_matches > 1:
            confidence = min(0.98, confidence + 0.05)
        
        logger.info(f"üéØ NLU Analysis Results:")
        logger.info(f"  Intent: {detected_intent} (confidence: {confidence:.2f})")
        logger.info(f"  Entities: {detected_entities}")
        logger.info(f"  Query Type: {query_type}")
        logger.info(f"  Sentiment: {sentiment}, Urgency: {urgency}")
        
        return TextAnalysisResult(
            intent=detected_intent,
            entities=detected_entities,
            context_markers=context_markers,
            preference_indicators=preference_indicators,
            query_type=query_type,
            confidence_score=confidence,
            sentiment=sentiment,
            urgency_level=urgency,
            processing_time_ms=0.0
        )

class ContextFusionEngine:
    """G√∂rsel ve metin analizlerini birle≈ütiren baƒülam f√ºzyon motoru"""
    
    def __init__(self):
        """ContextFusionEngine sƒ±nƒ±fƒ±nƒ± ba≈ülatƒ±r"""
        logger.info("üîÑ Context Fusion Engine initializing...")
        self.fusion_strategies = {
            QueryType.SHIRT_COMBINATION: self._fuse_shirt_combination,
            QueryType.DRESS_SHOE_MATCHING: self._fuse_dress_shoe_matching,
            QueryType.PANTS_JACKET_PAIRING: self._fuse_pants_jacket_pairing,
            QueryType.BAG_OUTFIT_STYLING: self._fuse_bag_outfit_styling,
            QueryType.GENERAL_STYLING: self._fuse_general_styling
        }
        logger.info("‚úÖ Context Fusion Engine initialized successfully")
    
    def fuse_contexts(self, 
                     visual_analysis: ImageAnalysisResult, 
                     textual_analysis: TextAnalysisResult,
                     user_profile: Dict[str, Any]) -> FusedContext:
        """
        G√∂rsel ve metin analizlerini birle≈ütirerek unified context olu≈üturur
        
        Args:
            visual_analysis: G√∂rsel analiz sonucu
            textual_analysis: Metin analiz sonucu
            user_profile: Kullanƒ±cƒ± profil bilgileri
            
        Returns:
            FusedContext: Birle≈ütirilmi≈ü baƒülam
        """
        logger.info("üîÄ Starting context fusion process...")
        
        try:
            # Query type'a g√∂re uygun fusion strategy'yi se√ß
            fusion_strategy = self.fusion_strategies.get(
                textual_analysis.query_type, 
                self._fuse_general_styling
            )
            
            # Fusion strategy'yi uygula
            fused_context = fusion_strategy(visual_analysis, textual_analysis, user_profile)
            
            # Fusion confidence hesapla
            fused_context.fusion_confidence = self._calculate_fusion_confidence(
                visual_analysis, textual_analysis
            )
            
            logger.info(f"‚úÖ Context fusion completed with confidence: {fused_context.fusion_confidence:.2f}")
            logger.info(f"üéØ Unified intent: {fused_context.unified_intent}")
            
            return fused_context
            
        except Exception as e:
            logger.error(f"‚ùå Context fusion failed: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Context fusion failed: {str(e)}")
    
    def _fuse_shirt_combination(self, visual: ImageAnalysisResult, textual: TextAnalysisResult, profile: Dict) -> FusedContext:
        """G√∂mlek kombinasyon sorgularƒ± i√ßin √∂zel fusion logic"""
        return FusedContext(
            visual_context=visual,
            textual_context=textual,
            user_profile=profile,
            unified_intent=f"Kullanƒ±cƒ± {visual.dominant_color} renkte {visual.style_category} tarzƒ±nda bir g√∂mlek i√ßin kombin √∂nerisi istiyor",
            recommendation_type="alt_par√ßa_ve_aksesuar_√∂nerisi",
            fusion_confidence=0.0,  # Sonradan hesaplanacak
            contextual_priorities=["renk_uyumu", "stil_tutarlƒ±lƒ±ƒüƒ±", "formallik_uyumu"],
            processing_metadata={
                "primary_focus": "g√∂mlek",
                "secondary_focus": "alt_par√ßa",
                "tertiary_focus": "aksesuar"
            }
        )
    
    def _fuse_dress_shoe_matching(self, visual: ImageAnalysisResult, textual: TextAnalysisResult, profile: Dict) -> FusedContext:
        """Elbise-ayakkabƒ± uyum sorgularƒ± i√ßin √∂zel fusion logic"""
        return FusedContext(
            visual_context=visual,
            textual_context=textual,
            user_profile=profile,
            unified_intent=f"Kullanƒ±cƒ± {visual.dominant_color} renkte {visual.style_category} elbiseye uygun ayakkabƒ± arƒ±yor",
            recommendation_type="ayakkabƒ±_uyum_√∂nerisi",
            fusion_confidence=0.0,
            contextual_priorities=["renk_uyumu", "formality_matching", "ocasyon_uygunluƒüu"],
            processing_metadata={
                "primary_focus": "elbise",
                "secondary_focus": "ayakkabƒ±",
                "style_constraints": visual.formality_level.value
            }
        )
    
    def _fuse_pants_jacket_pairing(self, visual: ImageAnalysisResult, textual: TextAnalysisResult, profile: Dict) -> FusedContext:
        """Pantolon-ceket kombinasyon sorgularƒ± i√ßin √∂zel fusion logic"""
        return FusedContext(
            visual_context=visual,
            textual_context=textual,
            user_profile=profile,
            unified_intent=f"Kullanƒ±cƒ± {visual.dominant_color} renkte pantolonla uyumlu ceket √∂nerisi istiyor",
            recommendation_type="ceket_√∂nerisi",
            fusion_confidence=0.0,
            contextual_priorities=["stil_uyumu", "renk_uyumu", "proportion_balance"],
            processing_metadata={
                "primary_focus": "pantolon",
                "secondary_focus": "ceket",
                "style_considerations": ["formal_casual_balance", "color_theory"]
            }
        )
    
    def _fuse_bag_outfit_styling(self, visual: ImageAnalysisResult, textual: TextAnalysisResult, profile: Dict) -> FusedContext:
        """√áanta merkezli styling sorgularƒ± i√ßin √∂zel fusion logic"""
        return FusedContext(
            visual_context=visual,
            textual_context=textual,
            user_profile=profile,
            unified_intent=f"Kullanƒ±cƒ± {visual.dominant_color} renkte √ßantaya uygun komple outfit istiyor",
            recommendation_type="komple_outfit_√∂nerisi",
            fusion_confidence=0.0,
            contextual_priorities=["√ßanta_uyumu", "head_to_toe_balance", "ocasyon_uygunluƒüu"],
            processing_metadata={
                "primary_focus": "√ßanta",
                "secondary_focus": "komple_outfit",
                "styling_approach": "√ßanta_merkezli"
            }
        )
    
    def _fuse_general_styling(self, visual: ImageAnalysisResult, textual: TextAnalysisResult, profile: Dict) -> FusedContext:
        """Genel styling sorgularƒ± i√ßin default fusion logic"""
        return FusedContext(
            visual_context=visual,
            textual_context=textual,
            user_profile=profile,
            unified_intent="Kullanƒ±cƒ± genel stil √∂nerisi arƒ±yor",
            recommendation_type="genel_stil_√∂nerisi",
            fusion_confidence=0.0,
            contextual_priorities=["kullanƒ±cƒ±_tercihleri", "trend_uyumu", "praktiklik"],
            processing_metadata={
                "approach": "genel",
                "flexibility": "y√ºksek"
            }
        )
    
    def _calculate_fusion_confidence(self, visual: ImageAnalysisResult, textual: TextAnalysisResult) -> float:
        """
        G√∂rsel ve metin analizlerinin g√ºven skorlarƒ±na g√∂re fusion confidence hesaplar
        """
        # Aƒüƒ±rlƒ±klƒ± ortalama ile confidence hesaplama
        visual_weight = 0.6  # G√∂rsel analiz biraz daha aƒüƒ±rlƒ±klƒ±
        textual_weight = 0.4
        
        fusion_confidence = (visual.confidence_score * visual_weight + 
                           textual.confidence_score * textual_weight)
        
        # 0.1 ile 1.0 arasƒ±nda clamp et
        return max(0.1, min(1.0, fusion_confidence))

# Request/Response models i√ßin Pydantic sƒ±nƒ±flarƒ±
class MultiModalQueryRequest(BaseModel):
    """√áok modlu sorgu isteƒüi i√ßin model"""
    image_base64: str = Field(..., description="Base64 encoded image data")
    text_query: str = Field(..., description="User's text query", min_length=1, max_length=500)
    user_id: Optional[str] = Field(None, description="User ID for personalization")
    context: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Additional context")

class MultiModalQueryResponse(BaseModel):
    """√áok modlu sorgu yanƒ±tƒ± i√ßin model"""
    success: bool
    query_id: str
    unified_intent: str
    recommendation_type: str
    visual_analysis: Dict[str, Any]
    textual_analysis: Dict[str, Any]
    fusion_confidence: float
    recommendations: List[Dict[str, Any]]
    processing_time_ms: float
    timestamp: str

# Ana koordinat√∂r sƒ±nƒ±fƒ±
class MultiModalCoordinator:
    """AURA AI √áok Modlu Sorgu Koordinat√∂r√º - Ana sƒ±nƒ±f"""
    
    def __init__(self):
        """MultiModalCoordinator sƒ±nƒ±fƒ±nƒ± ba≈ülatƒ±r ve t√ºm bile≈üenleri hazƒ±rlar"""
        logger.info("üéØ Multi-Modal Coordinator initializing...")
        
        # Bile≈üenleri ba≈ülat
        self.clip_processor = CLIPImageProcessor()
        self.nlu_processor = NLUTextProcessor()
        self.fusion_engine = ContextFusionEngine()
        
        # Service client'larƒ± ba≈ülat
        self.service_clients = {
            "style_profile": "http://localhost:8003",
            "combination_engine": "http://localhost:8004", 
            "recommendation_engine": "http://localhost:8005",
            "quality_assurance": "http://localhost:8008"
        }
        
        self.http_client = httpx.AsyncClient(timeout=30.0)
        
        logger.info("‚úÖ Multi-Modal Coordinator initialized successfully")
    
    async def process_multimodal_query(self, request: MultiModalQueryRequest) -> MultiModalQueryResponse:
        """
        √áok modlu sorguyu i≈üleyen ana metod
        
        Args:
            request: √áok modlu sorgu isteƒüi
            
        Returns:
            MultiModalQueryResponse: ƒ∞≈ülenmi≈ü sorgu yanƒ±tƒ±
        """
        start_time = datetime.now()
        query_id = f"mmq_{int(start_time.timestamp())}_{hash(request.text_query) % 10000}"
        
        logger.info(f"üöÄ Processing multi-modal query {query_id}")
        logger.info(f"üìù Text query: '{request.text_query}'")
        
        try:
            # 1. Base64 image'ƒ± decode et
            image_data = base64.b64decode(request.image_base64)
            
            # 2. Parallel processing - G√∂rsel ve metin analizini aynƒ± anda ba≈ülat
            image_task = self.clip_processor.analyze_image(image_data, request.text_query)
            text_task = self.nlu_processor.analyze_text(request.text_query)
            
            # 3. Her iki analizi tamamlanmasƒ±nƒ± bekle
            visual_analysis, textual_analysis = await asyncio.gather(image_task, text_task)
            
            # 4. Kullanƒ±cƒ± profilini al (eƒüer user_id varsa)
            user_profile = await self._get_user_profile(request.user_id) if request.user_id else {}
            
            # 5. Context fusion - Analizleri birle≈ütir
            fused_context = self.fusion_engine.fuse_contexts(
                visual_analysis, textual_analysis, user_profile
            )
            
            # 6. Service koordinasyonu ile √∂nerileri al
            recommendations = await self._coordinate_recommendation_services(fused_context)
            
            # 7. Kalite kontrol√º yap
            validated_recommendations = await self._validate_recommendations(recommendations, fused_context)
            
            # 8. Response hazƒ±rla
            total_processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            response = MultiModalQueryResponse(
                success=True,
                query_id=query_id,
                unified_intent=fused_context.unified_intent,
                recommendation_type=fused_context.recommendation_type,
                visual_analysis={
                    "item_type": visual_analysis.item_type,
                    "dominant_color": visual_analysis.dominant_color,
                    "style_category": visual_analysis.style_category,
                    "confidence": visual_analysis.confidence_score
                },
                textual_analysis={
                    "intent": textual_analysis.intent,
                    "entities": textual_analysis.entities,
                    "query_type": textual_analysis.query_type.value,
                    "confidence": textual_analysis.confidence_score
                },
                fusion_confidence=fused_context.fusion_confidence,
                recommendations=validated_recommendations,
                processing_time_ms=total_processing_time,
                timestamp=start_time.isoformat()
            )
            
            logger.info(f"‚úÖ Multi-modal query {query_id} completed in {total_processing_time:.2f}ms")
            logger.info(f"üéØ Generated {len(validated_recommendations)} recommendations")
            
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Multi-modal query {query_id} failed: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Multi-modal query processing failed: {str(e)}")
    
    async def _get_user_profile(self, user_id: str) -> Dict[str, Any]:
        """Kullanƒ±cƒ± profilini Style Profile servisinden alƒ±r"""
        try:
            response = await self.http_client.get(f"{self.service_clients['style_profile']}/profile/{user_id}")
            if response.status_code == 200:
                return response.json()
            else:
                logger.warning(f"‚ö†Ô∏è Could not fetch user profile for {user_id}")
                return {}
        except Exception as e:
            logger.error(f"‚ùå Error fetching user profile: {str(e)}")
            return {}
    
    async def _coordinate_recommendation_services(self, context: FusedContext) -> List[Dict[str, Any]]:
        """Fused context'e g√∂re appropriate servisleri koordine eder"""
        logger.info(f"üîÑ Coordinating services for {context.recommendation_type}")
        
        recommendations = []
        
        try:
            # Query type'a g√∂re farklƒ± service coordination strategies
            if context.textual_context.query_type == QueryType.SHIRT_COMBINATION:
                recommendations = await self._handle_shirt_combination(context)
            elif context.textual_context.query_type == QueryType.DRESS_SHOE_MATCHING:
                recommendations = await self._handle_dress_shoe_matching(context)
            elif context.textual_context.query_type == QueryType.PANTS_JACKET_PAIRING:
                recommendations = await self._handle_pants_jacket_pairing(context)
            elif context.textual_context.query_type == QueryType.BAG_OUTFIT_STYLING:
                recommendations = await self._handle_bag_outfit_styling(context)
            else:
                recommendations = await self._handle_general_styling(context)
            
            logger.info(f"üì¶ Generated {len(recommendations)} initial recommendations")
            return recommendations
            
        except Exception as e:
            logger.error(f"‚ùå Service coordination failed: {str(e)}")
            # Fallback √∂nerileri d√∂nd√ºr
            return await self._generate_fallback_recommendations(context)
    
    async def _handle_shirt_combination(self, context: FusedContext) -> List[Dict[str, Any]]:
        """G√∂mlek kombinasyon √∂nerileri i√ßin √∂zel koordinasyon"""
        try:
            # Combination Engine'e istek g√∂nder
            combination_request = {
                "base_item": {
                    "type": context.visual_context.item_type,
                    "color": context.visual_context.dominant_color,
                    "style": context.visual_context.style_category
                },
                "request_type": "bottom_piece_recommendation",
                "user_preferences": context.user_profile.get("preferences", {}),
                "formality_level": context.visual_context.formality_level.value
            }
            
            response = await self.http_client.post(
                f"{self.service_clients['combination_engine']}/recommend_combination",
                json=combination_request
            )
            
            if response.status_code == 200:
                combination_data = response.json()
                return combination_data.get("recommendations", [])
            else:
                logger.warning("‚ö†Ô∏è Combination Engine request failed, using fallback")
                return self._mock_shirt_recommendations(context)
                
        except Exception as e:
            logger.error(f"‚ùå Shirt combination handling failed: {str(e)}")
            return self._mock_shirt_recommendations(context)
    
    async def _handle_dress_shoe_matching(self, context: FusedContext) -> List[Dict[str, Any]]:
        """Elbise-ayakkabƒ± uyum √∂nerileri i√ßin √∂zel koordinasyon"""
        # Mock implementation - ger√ßek service koordinasyonu burada olacak
        return [
            {
                "type": "ayakkabƒ±_√∂nerisi",
                "item": "Siyah stiletto",
                "reason": f"Bu {context.visual_context.dominant_color} elbise ile m√ºkemmel uyum saƒülar",
                "confidence": 0.88,
                "style_notes": "Formal etkinlikler i√ßin ideal se√ßim"
            },
            {
                "type": "ayakkabƒ±_√∂nerisi", 
                "item": "Nude block heel",
                "reason": "Renk uyumu ve konfor dengesini saƒülar",
                "confidence": 0.85,
                "style_notes": "G√ºnl√ºk ≈üƒ±klƒ±k i√ßin m√ºkemmel"
            }
        ]
    
    async def _handle_pants_jacket_pairing(self, context: FusedContext) -> List[Dict[str, Any]]:
        """Pantolon-ceket kombinasyon √∂nerileri i√ßin √∂zel koordinasyon"""
        # Mock implementation
        return [
            {
                "type": "ceket_√∂nerisi",
                "item": "Lacivert blazer",
                "reason": f"Bu {context.visual_context.dominant_color} pantolon ile klasik uyum",
                "confidence": 0.91,
                "style_notes": "Business casual i√ßin ideal"
            }
        ]
    
    async def _handle_bag_outfit_styling(self, context: FusedContext) -> List[Dict[str, Any]]:
        """√áanta merkezli komple outfit √∂nerileri i√ßin √∂zel koordinasyon"""
        # Mock implementation
        return [
            {
                "type": "komple_outfit",
                "items": ["Beyaz g√∂mlek", "Siyah pantolon", "Siyah stiletto"],
                "reason": f"Bu {context.visual_context.dominant_color} √ßanta ile profesyonel look",
                "confidence": 0.87,
                "style_notes": "Ofis ve ak≈üam etkinlikleri i√ßin uygun"
            }
        ]
    
    async def _handle_general_styling(self, context: FusedContext) -> List[Dict[str, Any]]:
        """Genel styling √∂nerileri i√ßin koordinasyon"""
        # Mock implementation
        return [
            {
                "type": "genel_stil_√∂nerisi",
                "suggestion": "Minimalist ve ≈üƒ±k bir kombin",
                "reason": "Kullanƒ±cƒ± profiline uygun genel √∂neri",
                "confidence": 0.75,
                "style_notes": "√áok ama√ßlƒ± kullanƒ±m i√ßin uygundur"
            }
        ]
    
    def _mock_shirt_recommendations(self, context: FusedContext) -> List[Dict[str, Any]]:
        """G√∂mlek i√ßin fallback √∂nerileri"""
        return [
            {
                "type": "alt_par√ßa_√∂nerisi",
                "item": "Siyah kuma≈ü pantolon", 
                "reason": f"Bu {context.visual_context.dominant_color} g√∂mlek ile professional look yaratƒ±r",
                "confidence": 0.85,
                "style_notes": "ƒ∞≈ü ve sosyal etkinlikler i√ßin ideal"
            },
            {
                "type": "alt_par√ßa_√∂nerisi",
                "item": "Lacivert chino pantolon",
                "reason": "Smart casual look i√ßin m√ºkemmel se√ßim",
                "confidence": 0.82,
                "style_notes": "Rahat ve ≈üƒ±k g√ºnl√ºk kombinler i√ßin"
            }
        ]
    
    async def _generate_fallback_recommendations(self, context: FusedContext) -> List[Dict[str, Any]]:
        """Genel fallback √∂nerileri √ºretir"""
        return [
            {
                "type": "fallback_√∂neri",
                "suggestion": "Temel kombin √∂nerisi",
                "reason": "G√ºvenli ve klasik se√ßim",
                "confidence": 0.70,
                "style_notes": "Her zaman uygun olan temel kombinasyon"
            }
        ]
    
    async def _validate_recommendations(self, recommendations: List[Dict[str, Any]], context: FusedContext) -> List[Dict[str, Any]]:
        """Quality Assurance servisi ile √∂nerileri doƒürular"""
        try:
            # Quality Assurance servisine validation isteƒüi g√∂nder
            validation_request = {
                "ai_output": {
                    "recommendations": recommendations,
                    "context": context.unified_intent
                },
                "context": {
                    "service_source": "multi_modal_coordinator",
                    "query_type": context.textual_context.query_type.value,
                    "fusion_confidence": context.fusion_confidence
                }
            }
            
            response = await self.http_client.post(
                f"{self.service_clients['quality_assurance']}/validate",
                json=validation_request
            )
            
            if response.status_code == 200:
                validation_result = response.json()
                if validation_result.get("status") == "approved":
                    logger.info("‚úÖ Recommendations validated successfully")
                    return recommendations
                else:
                    logger.warning("‚ö†Ô∏è Recommendations need improvement")
                    # Improvement suggestions'larƒ± apply et
                    return self._apply_quality_improvements(recommendations, validation_result)
            else:
                logger.warning("‚ö†Ô∏è Quality validation failed, returning original recommendations")
                return recommendations
                
        except Exception as e:
            logger.error(f"‚ùå Quality validation failed: {str(e)}")
            return recommendations
    
    def _apply_quality_improvements(self, recommendations: List[Dict[str, Any]], validation_result: Dict) -> List[Dict[str, Any]]:
        """Quality Assurance √∂nerilerine g√∂re recommendations'larƒ± improve eder"""
        # Basit implementation - ger√ßek projede daha sophisticated logic olacak
        improvements = validation_result.get("improvement_suggestions", [])
        
        # Her recommendation'a improvement note ekle
        for rec in recommendations:
            rec["quality_notes"] = improvements[:2] if improvements else []
            rec["validated"] = True
        
        return recommendations

# Ana service instance'ƒ±
coordinator = MultiModalCoordinator()

# Export edilecek ana fonksiyon
async def process_query(request: MultiModalQueryRequest) -> MultiModalQueryResponse:
    """Multi-modal query processing i√ßin ana entry point"""
    return await coordinator.process_multimodal_query(request)
