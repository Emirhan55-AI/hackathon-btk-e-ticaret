{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8a93e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kütüphaneler yüklendi ve yollar tanımlandı.\n",
      "Başlangıçtaki toplam veri sayısı: 44424\n",
      "\n",
      "Üzerinde çalışacağımız en yaygın 10 kategori:\n",
      "['Tshirts', 'Shirts', 'Casual Shoes', 'Watches', 'Sports Shoes', 'Kurtas', 'Tops', 'Handbags', 'Heels', 'Sunglasses']\n",
      "Filtreleme sonrası veri sayısı: 25468\n",
      "\n",
      "Kayıp resimler kontrol ediliyor...\n",
      "Kontrol tamamlandı. 3 adet kayıp resim veriden çıkarıldı.\n",
      "Temizleme sonrası kalan veri sayısı: 25465\n",
      "\n",
      "Veri, Eğitim (%80) ve Doğrulama (%20) setlerine ayrılıyor...\n",
      "Eğitim seti boyutu: 20372\n",
      "Doğrulama seti boyutu: 5093\n",
      "\n",
      "İşlem Tamamlandı!\n",
      "Eğitim verisi şuraya kaydedildi: ..\\data\\processed\\train_cleaned.csv\n",
      "Doğrulama verisi şuraya kaydedildi: ..\\data\\processed\\validation_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# AURA AI ENGINE - ADIM 3: VERİ ÖN İŞLEME VE TEMİZLEME\n",
    "# ==============================================================================\n",
    "# AMAÇ: Keşifsel veri analizi sırasında tespit edilen sorunları (dengesizlik,\n",
    "#       olası hatalar) ele alarak modeli eğitime hazırlamak.\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# --- ADIM 1: Kütüphanelerin Yüklenmesi ve Veri Yollarının Tanımlanması ---\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Veri yollarını tanımlıyoruz\n",
    "DATA_PATH = os.path.join(\"..\", \"data\", \"raw\")\n",
    "PROCESSED_DATA_PATH = os.path.join(\"..\", \"data\", \"processed\")\n",
    "CSV_PATH = os.path.join(DATA_PATH, \"styles.csv\")\n",
    "IMAGES_PATH = os.path.join(DATA_PATH, \"images\")\n",
    "\n",
    "# İşlenmiş verileri kaydedeceğimiz klasör yoksa oluştur\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "print(\"Kütüphaneler yüklendi ve yollar tanımlandı.\")\n",
    "\n",
    "\n",
    "# --- ADIM 2: Verinin Yüklenmesi ve Problemin Basitleştirilmesi ---\n",
    "\n",
    "# Ham veriyi yüklüyoruz\n",
    "try:\n",
    "    df_raw = pd.read_csv(CSV_PATH, on_bad_lines='skip')\n",
    "    print(f\"Başlangıçtaki toplam veri sayısı: {len(df_raw)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"HATA: {CSV_PATH} bulunamadı.\")\n",
    "    # Eğer dosya bulunamazsa kodun devam etmesini engelle\n",
    "    exit()\n",
    "\n",
    "# En yaygın 10 kategoriyi seçerek problemi basitleştiriyoruz\n",
    "top_10_categories = df_raw['articleType'].value_counts().head(10).index.tolist()\n",
    "print(f\"\\nÜzerinde çalışacağımız en yaygın 10 kategori:\\n{top_10_categories}\")\n",
    "\n",
    "# Veri setini sadece bu 10 kategoriyi içerecek şekilde filtreliyoruz\n",
    "df = df_raw[df_raw['articleType'].isin(top_10_categories)].copy()\n",
    "print(f\"Filtreleme sonrası veri sayısı: {len(df)}\")\n",
    "\n",
    "\n",
    "# --- ADIM 3: Kayıp Resimlerin Temizlenmesi ---\n",
    "\n",
    "print(\"\\nKayıp resimler kontrol ediliyor...\")\n",
    "\n",
    "# Her bir satır için ilgili resmin diskte var olup olmadığını kontrol edecek bir fonksiyon\n",
    "def check_image_exists(image_id):\n",
    "    image_path = os.path.join(IMAGES_PATH, str(image_id) + \".jpg\")\n",
    "    return os.path.exists(image_path)\n",
    "\n",
    "# Fonksiyonu tüm veri setine uygulayarak 'image_exists' adında yeni bir sütun oluşturuyoruz\n",
    "df['image_exists'] = df['id'].apply(check_image_exists)\n",
    "\n",
    "# Sadece resmi var olan satırları tutuyoruz\n",
    "missing_images_count = len(df[~df['image_exists']])\n",
    "df = df[df['image_exists']]\n",
    "\n",
    "print(f\"Kontrol tamamlandı. {missing_images_count} adet kayıp resim veriden çıkarıldı.\")\n",
    "print(f\"Temizleme sonrası kalan veri sayısı: {len(df)}\")\n",
    "\n",
    "\n",
    "# --- ADIM 4: Veriyi Eğitim ve Doğrulama Setlerine Ayırma ---\n",
    "\n",
    "print(\"\\nVeri, Eğitim (%80) ve Doğrulama (%20) setlerine ayrılıyor...\")\n",
    "\n",
    "# 'train_test_split' fonksiyonu ile veriyi ayırıyoruz.\n",
    "# stratify=df['articleType'] parametresi ÇOK ÖNEMLİ: Bu, ayırma işlemi sırasında\n",
    "# her iki sette de kategori oranlarının korunmasını sağlar. (Dengesizliğe karşı ilk önlem)\n",
    "train_df, validation_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,          # Verinin %20'si doğrulama seti olacak\n",
    "    random_state=42,        # Sonuçların tekrarlanabilir olması için sabit bir başlangıç noktası\n",
    "    stratify=df['articleType'] # Kategori oranlarını koru\n",
    ")\n",
    "\n",
    "print(f\"Eğitim seti boyutu: {len(train_df)}\")\n",
    "print(f\"Doğrulama seti boyutu: {len(validation_df)}\")\n",
    "\n",
    "\n",
    "# --- ADIM 5: Temizlenmiş ve Ayrılmış Veriyi Kaydetme ---\n",
    "\n",
    "# Gelecekteki adımlarda bu işlemleri tekrar yapmamak için temizlenmiş\n",
    "# veri setlerini 'processed' klasörüne yeni CSV dosyaları olarak kaydediyoruz.\n",
    "train_save_path = os.path.join(PROCESSED_DATA_PATH, \"train_cleaned.csv\")\n",
    "validation_save_path = os.path.join(PROCESSED_DATA_PATH, \"validation_cleaned.csv\")\n",
    "\n",
    "train_df.to_csv(train_save_path, index=False)\n",
    "validation_df.to_csv(validation_save_path, index=False)\n",
    "\n",
    "print(f\"\\nİşlem Tamamlandı!\")\n",
    "print(f\"Eğitim verisi şuraya kaydedildi: {train_save_path}\")\n",
    "print(f\"Doğrulama verisi şuraya kaydedildi: {validation_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aura_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
