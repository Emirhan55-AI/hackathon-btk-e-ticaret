# âš¡ AURA AI SÄ°STEMÄ° - PERFORMANS TEST PAKETÄ°
# Test OdaklÄ± Geri Besleme DÃ¶ngÃ¼sÃ¼ (AlphaCodium/SED) Performans Benchmark Sistemi

import pytest
import time
import asyncio
import statistics
import threading
import concurrent.futures
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
import json
import psutil
import requests
from unittest.mock import Mock, patch

# Test framework konfigÃ¼rasyonunu import et
from ..conftest import AuraTestConfig, TestUtilities

class PerformanceTestSuite:
    """
    Performans test paketi.
    
    Bu sÄ±nÄ±f, sistemin Ã§eÅŸitli yÃ¼k koÅŸullarÄ±ndaki performansÄ±nÄ± Ã¶lÃ§er.
    YanÄ±t sÃ¼releri, throughput, kaynak kullanÄ±mÄ± ve scalability testleri yapar.
    """
    
    def __init__(self):
        # Test konfigÃ¼rasyonunu yÃ¼kle
        self.config = AuraTestConfig()
        self.utils = TestUtilities()
        
        # Performans kriterleri ve thresholdlar
        self.performance_thresholds = {
            'response_time_ms': {
                'excellent': 200,
                'good': 500,
                'acceptable': 1000,
                'poor': 2000,
                'unacceptable': 5000
            },
            'throughput_rps': {
                'minimum': 10,
                'target': 50,  
                'excellent': 100
            },
            'concurrent_users': {
                'minimum': 10,
                'target': 50,
                'maximum': 200
            },
            'resource_usage': {
                'cpu_percent': 80,
                'memory_mb': 1024,
                'disk_io_mbps': 100
            }
        }
        
        # Test sonuÃ§larÄ±nÄ± saklamak iÃ§in
        self.performance_results = {
            'response_time_tests': {},
            'load_tests': {},
            'stress_tests': {},
            'endurance_tests': {},
            'resource_usage_tests': {}
        }

# Test sÄ±nÄ±flarÄ±
class TestResponseTimeBenchmarks:
    """
    YanÄ±t sÃ¼resi benchmark testleri.
    
    Bu test grubu, her servisin farklÄ± yÃ¼k koÅŸullarÄ±ndaki yanÄ±t sÃ¼relerini Ã¶lÃ§er.
    """
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """Her test Ã¶ncesi gerekli kurulumlarÄ± yap"""
        self.config = AuraTestConfig()
        self.utils = TestUtilities()
        
        print("\nâš¡ YanÄ±t sÃ¼resi benchmark testleri baÅŸlatÄ±lÄ±yor...")
    
    def test_single_request_response_times(self):
        """
        Tek istek yanÄ±t sÃ¼relerini test et.
        
        Her servise tek istek gÃ¶nderip yanÄ±t sÃ¼relerini Ã¶lÃ§.
        Bu baseline performans metriÄŸidir.
        """
        print("   ğŸ” Test: Tek istek yanÄ±t sÃ¼releri")
        
        # Her servis iÃ§in yanÄ±t sÃ¼resi Ã¶lÃ§
        response_times = {}
        
        for service_name, service_url in self.config.SERVICES.items():
            print(f"      ğŸ“Š {service_name} yanÄ±t sÃ¼resi Ã¶lÃ§Ã¼lÃ¼yor...")
            
            # 10 kez Ã¶lÃ§Ã¼m yapÄ±p ortalama al
            measurements = []
            
            for i in range(10):
                start_time = time.time()
                
                try:
                    # Mock request simÃ¼lasyonu
                    response = self._simulate_service_request(service_name, service_url)
                    response_time_ms = (time.time() - start_time) * 1000
                    measurements.append(response_time_ms)
                    
                except Exception as e:
                    print(f"         âš ï¸ Ã–lÃ§Ã¼m {i+1} baÅŸarÄ±sÄ±z: {str(e)}")
                    measurements.append(5000)  # Timeout deÄŸeri
            
            # Ä°statistikleri hesapla
            response_times[service_name] = {
                'avg_ms': statistics.mean(measurements),
                'min_ms': min(measurements),
                'max_ms': max(measurements),
                'median_ms': statistics.median(measurements),
                'std_dev': statistics.stdev(measurements) if len(measurements) > 1 else 0,
                'measurements': measurements
            }
            
            avg_time = response_times[service_name]['avg_ms']
            performance_level = self._categorize_response_time(avg_time)
            
            print(f"         â±ï¸ Ortalama: {avg_time:.2f}ms ({performance_level})")
            print(f"         ğŸ“ˆ Min: {response_times[service_name]['min_ms']:.2f}ms")
            print(f"         ğŸ“‰ Max: {response_times[service_name]['max_ms']:.2f}ms")
        
        # Genel deÄŸerlendirme
        total_avg = statistics.mean([rt['avg_ms'] for rt in response_times.values()])
        overall_performance = self._categorize_response_time(total_avg)
        
        print(f"\n      ğŸ¯ Genel ortalama yanÄ±t sÃ¼resi: {total_avg:.2f}ms ({overall_performance})")
        
        # DoÄŸrulamalar
        slow_services = [name for name, data in response_times.items() 
                        if data['avg_ms'] > self.config.PERFORMANCE_THRESHOLDS['response_time_ms']]
        
        assert len(slow_services) == 0, f"YavaÅŸ servisler tespit edildi: {slow_services}"
        assert total_avg < 1000, f"Genel ortalama yanÄ±t sÃ¼resi Ã§ok yÃ¼ksek: {total_avg:.2f}ms"
        
        return response_times
    
    def test_concurrent_request_response_times(self):
        """
        EÅŸzamanlÄ± istek yanÄ±t sÃ¼relerini test et.
        
        AynÄ± anda birden fazla istek gÃ¶nderip yanÄ±t sÃ¼relerinin nasÄ±l etkilendiÄŸini Ã¶lÃ§.
        """
        print("   ğŸ” Test: EÅŸzamanlÄ± istek yanÄ±t sÃ¼releri")
        
        concurrent_levels = [5, 10, 20, 50]  # EÅŸzamanlÄ±lÄ±k seviyeleri
        concurrent_results = {}
        
        for concurrent_count in concurrent_levels:
            print(f"      ğŸ“Š {concurrent_count} eÅŸzamanlÄ± istek testi...")
            
            # EÅŸzamanlÄ± istekler iÃ§in thread pool
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_count) as executor:
                start_time = time.time()
                
                # TÃ¼m istekleri baÅŸlat
                futures = []
                for i in range(concurrent_count):
                    service_name = list(self.config.SERVICES.keys())[i % len(self.config.SERVICES)]
                    service_url = self.config.SERVICES[service_name]
                    
                    future = executor.submit(self._simulate_service_request, service_name, service_url)
                    futures.append((future, service_name))
                
                # SonuÃ§larÄ± topla
                results = []
                for future, service_name in futures:
                    try:
                        future.result(timeout=10)  # 10 saniye timeout
                        request_time = time.time() - start_time
                        results.append({'service': service_name, 'time_ms': request_time * 1000})
                    except Exception as e:
                        results.append({'service': service_name, 'time_ms': 10000, 'error': str(e)})
                
                total_time = time.time() - start_time
                
                # Ä°statistikleri hesapla
                successful_requests = [r for r in results if 'error' not in r]
                failed_requests = [r for r in results if 'error' in r]
                
                concurrent_results[concurrent_count] = {
                    'total_time_ms': total_time * 1000,
                    'successful_count': len(successful_requests),
                    'failed_count': len(failed_requests),
                    'success_rate': len(successful_requests) / concurrent_count * 100,
                    'avg_response_time_ms': statistics.mean([r['time_ms'] for r in successful_requests]) if successful_requests else 0,
                    'throughput_rps': concurrent_count / total_time if total_time > 0 else 0
                }
                
                result = concurrent_results[concurrent_count]
                print(f"         âœ… BaÅŸarÄ±lÄ±: {result['successful_count']}/{concurrent_count}")
                print(f"         â±ï¸ Ortalama yanÄ±t: {result['avg_response_time_ms']:.2f}ms")
                print(f"         ğŸš€ Throughput: {result['throughput_rps']:.2f} req/sec")
                print(f"         ğŸ“Š BaÅŸarÄ± oranÄ±: %{result['success_rate']:.1f}")
        
        # Performans degradasyonunu analiz et
        baseline_throughput = concurrent_results[5]['throughput_rps'] if 5 in concurrent_results else 0
        
        for level, result in concurrent_results.items():
            if level > 5:
                degradation = ((baseline_throughput - result['throughput_rps']) / baseline_throughput) * 100 if baseline_throughput > 0 else 0
                print(f"      ğŸ“‰ {level} eÅŸzamanlÄ± istek degradasyonu: %{degradation:.1f}")
        
        # DoÄŸrulamalar
        for level, result in concurrent_results.items():
            assert result['success_rate'] >= 90, f"{level} eÅŸzamanlÄ± istekte baÅŸarÄ± oranÄ± dÃ¼ÅŸÃ¼k: %{result['success_rate']:.1f}"
            assert result['throughput_rps'] >= 5, f"{level} eÅŸzamanlÄ± istekte throughput Ã§ok dÃ¼ÅŸÃ¼k: {result['throughput_rps']:.2f}"
        
        return concurrent_results
    
    def _simulate_service_request(self, service_name: str, service_url: str) -> Dict:
        """Servis isteÄŸini simÃ¼le et"""
        # Mock response simÃ¼lasyonu
        simulation_delay = {
            'image_processing_service': 0.15,  # 150ms
            'nlu_service': 0.12,              # 120ms
            'style_profile_service': 0.08,    # 80ms
            'combination_engine_service': 0.25, # 250ms
            'recommendation_engine_service': 0.18, # 180ms
            'orchestrator_service': 0.3,      # 300ms
            'feedback_loop_service': 0.1      # 100ms
        }
        
        # GerÃ§ekÃ§i gecikme simÃ¼lasyonu
        delay = simulation_delay.get(service_name, 0.1)
        time.sleep(delay)
        
        return {
            "status": "success",
            "service": service_name,
            "timestamp": datetime.now().isoformat(),
            "simulated": True
        }
    
    def _categorize_response_time(self, response_time_ms: float) -> str:
        """YanÄ±t sÃ¼resini kategorize et"""
        thresholds = {
            200: "mÃ¼kemmel",
            500: "iyi", 
            1000: "kabul edilebilir",
            2000: "zayÄ±f",
            float('inf'): "kabul edilemez"
        }
        
        for threshold, category in thresholds.items():
            if response_time_ms <= threshold:
                return category
        
        return "bilinmeyen"

class TestLoadAndStressTesting:
    """
    YÃ¼k ve stres testleri.
    
    Bu test grubu, sistemin yÃ¼ksek yÃ¼k altÄ±ndaki davranÄ±ÅŸÄ±nÄ± test eder.
    """
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """Her test Ã¶ncesi gerekli kurulumlarÄ± yap"""
        self.config = AuraTestConfig()
        self.utils = TestUtilities()
        
        print("\nğŸ‹ï¸ YÃ¼k ve stres testleri baÅŸlatÄ±lÄ±yor...")
    
    def test_load_testing_sustained_traffic(self):
        """
        SÃ¼rekli trafik yÃ¼k testi.
        
        5 dakika boyunca sÃ¼rekli istekler gÃ¶nder ve sistemin stabilitesini Ã¶lÃ§.
        """
        print("   ğŸ” Test: SÃ¼rekli trafik yÃ¼k testi (5 dakika)")
        
        test_duration_seconds = 30  # Test iÃ§in kÄ±saltÄ±lmÄ±ÅŸ sÃ¼re (gerÃ§ekte 300 olacak)
        requests_per_second = 10
        
        start_time = time.time()
        end_time = start_time + test_duration_seconds
        
        results = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'response_times': [],
            'error_details': []
        }
        
        print(f"      â±ï¸ {test_duration_seconds} saniye boyunca {requests_per_second} req/sec gÃ¶nderiliyor...")
        
        # Load test dÃ¶ngÃ¼sÃ¼
        while time.time() < end_time:
            cycle_start = time.time()
            
            # Bu saniyede gÃ¶nderilecek istekler
            for i in range(requests_per_second):
                request_start = time.time()
                
                try:
                    # Random servis seÃ§
                    service_name = list(self.config.SERVICES.keys())[i % len(self.config.SERVICES)]
                    service_url = self.config.SERVICES[service_name]
                    
                    # Ä°stek simÃ¼le et
                    response = self._simulate_service_request(service_name, service_url)
                    response_time = (time.time() - request_start) * 1000
                    
                    results['total_requests'] += 1
                    results['successful_requests'] += 1
                    results['response_times'].append(response_time)
                    
                except Exception as e:
                    results['total_requests'] += 1
                    results['failed_requests'] += 1
                    results['error_details'].append(str(e))
                
                # Rate limiting (saniyede belirlenen sayÄ±da istek)
                if i < requests_per_second - 1:
                    time.sleep(1.0 / requests_per_second)
            
            # Bir sonraki saniyeye kadar bekle
            cycle_time = time.time() - cycle_start
            if cycle_time < 1.0:
                time.sleep(1.0 - cycle_time)
        
        # SonuÃ§larÄ± analiz et
        actual_duration = time.time() - start_time
        actual_rps = results['total_requests'] / actual_duration
        success_rate = (results['successful_requests'] / results['total_requests']) * 100 if results['total_requests'] > 0 else 0
        
        avg_response_time = statistics.mean(results['response_times']) if results['response_times'] else 0
        p95_response_time = sorted(results['response_times'])[int(len(results['response_times']) * 0.95)] if results['response_times'] else 0
        
        print(f"      ğŸ“Š Test sonuÃ§larÄ±:")
        print(f"         Toplam istek: {results['total_requests']}")
        print(f"         BaÅŸarÄ±lÄ±: {results['successful_requests']} (%{success_rate:.1f})")
        print(f"         BaÅŸarÄ±sÄ±z: {results['failed_requests']}")
        print(f"         GerÃ§ek RPS: {actual_rps:.2f}")
        print(f"         Ortalama yanÄ±t: {avg_response_time:.2f}ms")
        print(f"         P95 yanÄ±t: {p95_response_time:.2f}ms")
        
        # DoÄŸrulamalar
        assert success_rate >= 95, f"BaÅŸarÄ± oranÄ± Ã§ok dÃ¼ÅŸÃ¼k: %{success_rate:.1f}"
        assert actual_rps >= requests_per_second * 0.8, f"Throughput target'Ä±n altÄ±nda: {actual_rps:.2f}"
        assert avg_response_time < 1000, f"Ortalama yanÄ±t sÃ¼resi Ã§ok yÃ¼ksek: {avg_response_time:.2f}ms"
        assert p95_response_time < 2000, f"P95 yanÄ±t sÃ¼resi Ã§ok yÃ¼ksek: {p95_response_time:.2f}ms"
        
        return results
    
    def test_stress_testing_breaking_point(self):
        """
        Stres testi - kÄ±rÄ±lma noktasÄ± bulma.
        
        GittikÃ§e artan yÃ¼kle sistemin kÄ±rÄ±lma noktasÄ±nÄ± bul.
        """
        print("   ğŸ” Test: Stres testi - kÄ±rÄ±lma noktasÄ±")
        
        stress_levels = [10, 25, 50, 75, 100, 150, 200]  # RPS deÄŸerleri
        stress_results = {}
        breaking_point = None
        
        for rps_level in stress_levels:
            print(f"      ğŸ“ˆ {rps_level} RPS stres seviyesi test ediliyor...")
            
            test_duration = 10  # Her seviye iÃ§in 10 saniye
            start_time = time.time()
            
            level_results = {
                'target_rps': rps_level,
                'total_requests': 0,
                'successful_requests': 0,
                'failed_requests': 0,
                'response_times': [],
                'errors': []
            }
            
            # Stress test dÃ¶ngÃ¼sÃ¼
            with concurrent.futures.ThreadPoolExecutor(max_workers=min(rps_level, 50)) as executor:
                futures = []
                
                # Belirtilen RPS'de istekler gÃ¶nder
                for second in range(test_duration):
                    for request_num in range(rps_level):
                        service_name = list(self.config.SERVICES.keys())[request_num % len(self.config.SERVICES)]
                        service_url = self.config.SERVICES[service_name]
                        
                        future = executor.submit(self._simulate_service_request, service_name, service_url)
                        futures.append(future)
                        level_results['total_requests'] += 1
                    
                    # Rate limiting
                    time.sleep(0.1)  # KÄ±sa sleep interval
                
                # TÃ¼m sonuÃ§larÄ± topla
                for future in concurrent.futures.as_completed(futures, timeout=30):
                    try:
                        result = future.result(timeout=5)
                        level_results['successful_requests'] += 1
                    except Exception as e:
                        level_results['failed_requests'] += 1
                        level_results['errors'].append(str(e))
            
            # Seviye sonuÃ§larÄ±nÄ± hesapla
            actual_duration = time.time() - start_time
            actual_rps = level_results['total_requests'] / actual_duration
            success_rate = (level_results['successful_requests'] / level_results['total_requests']) * 100 if level_results['total_requests'] > 0 else 0
            
            stress_results[rps_level] = {
                'actual_rps': actual_rps,
                'success_rate': success_rate,
                'failed_requests': level_results['failed_requests'],
                'error_count': len(level_results['errors'])
            }
            
            print(f"         âš¡ GerÃ§ek RPS: {actual_rps:.2f}")
            print(f"         âœ… BaÅŸarÄ± oranÄ±: %{success_rate:.1f}")
            print(f"         âŒ Hata sayÄ±sÄ±: {level_results['failed_requests']}")
            
            # KÄ±rÄ±lma noktasÄ±nÄ± belirle
            if success_rate < 90 or level_results['failed_requests'] > level_results['total_requests'] * 0.1:
                breaking_point = rps_level
                print(f"         ğŸ”¥ KÄ±rÄ±lma noktasÄ± tespit edildi: {rps_level} RPS")
                break
            
            # Rate limiting - bir sonraki seviye iÃ§in bekle
            time.sleep(2)
        
        # SonuÃ§larÄ± Ã¶zetle
        if breaking_point:
            print(f"\n      ğŸ¯ Sistem kÄ±rÄ±lma noktasÄ±: {breaking_point} RPS")
            recommended_max = int(breaking_point * 0.7)  # %70'i gÃ¼venli kabul et
            print(f"      ğŸ’¡ Ã–nerilen maksimum yÃ¼k: {recommended_max} RPS")
        else:
            print(f"\n      ğŸš€ Sistem {max(stress_levels)} RPS'e kadar stabil!")
        
        # DoÄŸrulamalar
        assert len(stress_results) > 0, "HiÃ§ stress testi tamamlanamadÄ±"
        
        # En az 50 RPS'i kaldÄ±rabilmeli
        stable_levels = [level for level, result in stress_results.items() if result['success_rate'] >= 90]
        assert len(stable_levels) > 0 and max(stable_levels) >= 50, f"Sistem 50 RPS'i kaldÄ±ramÄ±yor: {stable_levels}"
        
        return stress_results
    
    def _simulate_service_request(self, service_name: str, service_url: str) -> Dict:
        """Servis isteÄŸini simÃ¼le et (load test iÃ§in optimized)"""
        # Daha hÄ±zlÄ± simÃ¼lasyon iÃ§in kÄ±saltÄ±lmÄ±ÅŸ gecikmeler
        simulation_delay = {
            'image_processing_service': 0.05,   # 50ms
            'nlu_service': 0.03,               # 30ms
            'style_profile_service': 0.02,     # 20ms
            'combination_engine_service': 0.08, # 80ms
            'recommendation_engine_service': 0.06, # 60ms
            'orchestrator_service': 0.1,       # 100ms
            'feedback_loop_service': 0.03      # 30ms
        }
        
        # GerÃ§ekÃ§i gecikme simÃ¼lasyonu
        delay = simulation_delay.get(service_name, 0.03)
        time.sleep(delay)
        
        return {
            "status": "success",
            "service": service_name,
            "timestamp": datetime.now().isoformat(),
            "simulated": True
        }

class TestResourceUsageMonitoring:
    """
    Kaynak kullanÄ±mÄ± izleme testleri.
    
    Bu test grubu, sistem kaynaklarÄ±nÄ±n (CPU, RAM, Disk) kullanÄ±mÄ±nÄ± izler.
    """
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """Her test Ã¶ncesi gerekli kurulumlarÄ± yap"""
        self.config = AuraTestConfig()
        self.utils = TestUtilities()
        
        print("\nğŸ“Š Kaynak kullanÄ±mÄ± izleme testleri baÅŸlatÄ±lÄ±yor...")
    
    def test_resource_usage_under_normal_load(self):
        """
        Normal yÃ¼k altÄ±nda kaynak kullanÄ±mÄ±nÄ± test et.
        
        Normal iÅŸlem yÃ¼kÃ¼ altÄ±nda CPU, RAM ve disk kullanÄ±mÄ±nÄ± izle.
        """
        print("   ğŸ” Test: Normal yÃ¼k altÄ±nda kaynak kullanÄ±mÄ±")
        
        # Baseline kaynak kullanÄ±mÄ±nÄ± Ã¶lÃ§
        baseline_stats = self._get_system_stats()
        print(f"      ğŸ“ˆ Baseline CPU: %{baseline_stats['cpu_percent']:.1f}")
        print(f"      ğŸ§  Baseline RAM: {baseline_stats['memory_mb']:.1f}MB")
        print(f"      ğŸ’¾ Baseline Disk: {baseline_stats['disk_io_mbps']:.2f}MB/s")
        
        # Normal yÃ¼k simÃ¼lasyonu (30 saniye)
        test_duration = 10  # Test iÃ§in kÄ±saltÄ±lmÄ±ÅŸ
        monitoring_interval = 1  # Her 1 saniyede Ã¶lÃ§Ã¼m
        
        resource_measurements = []
        
        print(f"      â±ï¸ {test_duration} saniye normal yÃ¼k simÃ¼lasyonu...")
        
        start_time = time.time()
        end_time = start_time + test_duration
        
        # Normal yÃ¼k thread'i baÅŸlat
        load_thread = threading.Thread(target=self._simulate_normal_load, args=(test_duration,))
        load_thread.start()
        
        # Kaynak kullanÄ±mÄ±nÄ± izle
        while time.time() < end_time:
            current_stats = self._get_system_stats()
            resource_measurements.append({
                'timestamp': time.time() - start_time,
                'cpu_percent': current_stats['cpu_percent'],
                'memory_mb': current_stats['memory_mb'],
                'disk_io_mbps': current_stats['disk_io_mbps']
            })
            
            time.sleep(monitoring_interval)
        
        # Load thread'in bitmesini bekle
        load_thread.join()
        
        # Ä°statistikleri hesapla
        if resource_measurements:
            avg_cpu = statistics.mean([m['cpu_percent'] for m in resource_measurements])
            max_cpu = max([m['cpu_percent'] for m in resource_measurements])
            avg_memory = statistics.mean([m['memory_mb'] for m in resource_measurements])
            max_memory = max([m['memory_mb'] for m in resource_measurements])
            avg_disk = statistics.mean([m['disk_io_mbps'] for m in resource_measurements])
            max_disk = max([m['disk_io_mbps'] for m in resource_measurements])
            
            print(f"      ğŸ“Š Test sonuÃ§larÄ±:")
            print(f"         Ortalama CPU: %{avg_cpu:.1f} (Max: %{max_cpu:.1f})")
            print(f"         Ortalama RAM: {avg_memory:.1f}MB (Max: {max_memory:.1f}MB)")
            print(f"         Ortalama Disk I/O: {avg_disk:.2f}MB/s (Max: {max_disk:.2f}MB/s)")
            
            # Performans deÄŸerlendirmesi
            cpu_status = "iyi" if avg_cpu < 50 else "orta" if avg_cpu < 80 else "yÃ¼ksek"
            memory_status = "iyi" if avg_memory < 512 else "orta" if avg_memory < 1024 else "yÃ¼ksek"
            
            print(f"         ğŸ¯ CPU kullanÄ±mÄ±: {cpu_status}")
            print(f"         ğŸ¯ RAM kullanÄ±mÄ±: {memory_status}")
        
            # DoÄŸrulamalar
            assert avg_cpu < 80, f"Ortalama CPU kullanÄ±mÄ± Ã§ok yÃ¼ksek: %{avg_cpu:.1f}"
            assert max_cpu < 95, f"Maksimum CPU kullanÄ±mÄ± Ã§ok yÃ¼ksek: %{max_cpu:.1f}"
            assert avg_memory < 1024, f"Ortalama RAM kullanÄ±mÄ± Ã§ok yÃ¼ksek: {avg_memory:.1f}MB"
            
            return {
                'avg_cpu': avg_cpu,
                'max_cpu': max_cpu,
                'avg_memory': avg_memory,
                'max_memory': max_memory,
                'measurements': resource_measurements
            }
        else:
            print("      âš ï¸ Kaynak Ã¶lÃ§Ã¼mÃ¼ alÄ±namadÄ±")
            return {}
    
    def test_resource_usage_under_heavy_load(self):
        """
        YoÄŸun yÃ¼k altÄ±nda kaynak kullanÄ±mÄ±nÄ± test et.
        
        YÃ¼ksek iÅŸlem yÃ¼kÃ¼ altÄ±nda kaynak kullanÄ±mÄ±nÄ± izle ve limitleri kontrol et.
        """
        print("   ğŸ” Test: YoÄŸun yÃ¼k altÄ±nda kaynak kullanÄ±mÄ±")
        
        test_duration = 15  # Test iÃ§in kÄ±saltÄ±lmÄ±ÅŸ
        monitoring_interval = 0.5  # Daha sÄ±k Ã¶lÃ§Ã¼m
        
        resource_measurements = []
        
        print(f"      âš¡ {test_duration} saniye yoÄŸun yÃ¼k simÃ¼lasyonu...")
        
        start_time = time.time()
        end_time = start_time + test_duration
        
        # YoÄŸun yÃ¼k thread'leri baÅŸlat
        heavy_load_threads = []
        for i in range(3):  # 3 parallel heavy load thread
            thread = threading.Thread(target=self._simulate_heavy_load, args=(test_duration,))
            thread.start()
            heavy_load_threads.append(thread)
        
        # Kaynak kullanÄ±mÄ±nÄ± izle
        while time.time() < end_time:
            current_stats = self._get_system_stats()
            resource_measurements.append({
                'timestamp': time.time() - start_time,
                'cpu_percent': current_stats['cpu_percent'],
                'memory_mb': current_stats['memory_mb'],
                'disk_io_mbps': current_stats['disk_io_mbps']
            })
            
            time.sleep(monitoring_interval)
        
        # TÃ¼m thread'lerin bitmesini bekle
        for thread in heavy_load_threads:
            thread.join()
        
        # Ä°statistikleri hesapla
        if resource_measurements:
            avg_cpu = statistics.mean([m['cpu_percent'] for m in resource_measurements])
            max_cpu = max([m['cpu_percent'] for m in resource_measurements])
            avg_memory = statistics.mean([m['memory_mb'] for m in resource_measurements])
            max_memory = max([m['memory_mb'] for m in resource_measurements])
            
            print(f"      ğŸ“Š YoÄŸun yÃ¼k test sonuÃ§larÄ±:")
            print(f"         Ortalama CPU: %{avg_cpu:.1f} (Max: %{max_cpu:.1f})")
            print(f"         Ortalama RAM: {avg_memory:.1f}MB (Max: {max_memory:.1f}MB)")
            
            # Kritik seviye uyarÄ±larÄ±
            if max_cpu > 90:
                print(f"         ğŸ”¥ UYARI: CPU kullanÄ±mÄ± kritik seviyede! %{max_cpu:.1f}")
            if max_memory > 2048:
                print(f"         ğŸ”¥ UYARI: RAM kullanÄ±mÄ± kritik seviyede! {max_memory:.1f}MB")
            
            # DoÄŸrulamalar (heavy load iÃ§in daha yÃ¼ksek thresholdlar)
            assert max_cpu < 98, f"CPU kullanÄ±mÄ± sistem limitini aÅŸtÄ±: %{max_cpu:.1f}"
            assert max_memory < 4096, f"RAM kullanÄ±mÄ± sistem limitini aÅŸtÄ±: {max_memory:.1f}MB"
            
            return {
                'avg_cpu': avg_cpu,
                'max_cpu': max_cpu,
                'avg_memory': avg_memory,
                'max_memory': max_memory,
                'peak_usage_detected': max_cpu > 85 or max_memory > 1536
            }
        else:
            print("      âš ï¸ Kaynak Ã¶lÃ§Ã¼mÃ¼ alÄ±namadÄ±")
            return {}
    
    def _get_system_stats(self) -> Dict:
        """Mevcut sistem istatistiklerini al"""
        try:
            # psutil ile gerÃ§ek sistem istatistikleri
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory_info = psutil.virtual_memory()
            memory_mb = memory_info.used / (1024 * 1024)
            
            # Disk I/O (basit simÃ¼lasyon)
            disk_io = psutil.disk_io_counters()
            disk_io_mbps = (disk_io.read_bytes + disk_io.write_bytes) / (1024 * 1024) if disk_io else 0
            
            return {
                'cpu_percent': cpu_percent,
                'memory_mb': memory_mb,
                'disk_io_mbps': disk_io_mbps,
                'timestamp': time.time()
            }
        except Exception as e:
            # Fallback mock data
            return {
                'cpu_percent': 25.0,
                'memory_mb': 256.0,
                'disk_io_mbps': 5.0,
                'timestamp': time.time(),
                'mock': True
            }
    
    def _simulate_normal_load(self, duration: int):
        """Normal yÃ¼k simÃ¼lasyonu"""
        end_time = time.time() + duration
        
        while time.time() < end_time:
            # Basit CPU iÅŸlemleri
            for i in range(1000):
                _ = i ** 2
            
            # KÄ±sa memory allocation
            temp_data = [i for i in range(1000)]
            del temp_data
            
            time.sleep(0.01)  # CPU'yu tamamen meÅŸgul etme
    
    def _simulate_heavy_load(self, duration: int):
        """YoÄŸun yÃ¼k simÃ¼lasyonu"""
        end_time = time.time() + duration
        
        while time.time() < end_time:
            # YoÄŸun CPU iÅŸlemleri
            for i in range(10000):
                _ = i ** 3
            
            # Memory allocation
            temp_data = [i for i in range(5000)]
            del temp_data
            
            time.sleep(0.001)  # Minimal sleep

# Ana test fonksiyonu
def run_performance_tests():
    """
    TÃ¼m performans testlerini Ã§alÄ±ÅŸtÄ±r.
    
    Bu fonksiyon, sistemin performans karakteristiklerini kapsamlÄ± bir ÅŸekilde test eder.
    """
    print("âš¡ PERFORMANS TEST PAKETÄ° BAÅLATILIYOR")
    print("=" * 60)
    
    test_results = {
        'total_tests': 0,
        'passed_tests': 0,
        'failed_tests': 0,
        'performance_metrics': {},
        'test_details': []
    }
    
    # TÃ¼m test sÄ±nÄ±flarÄ±nÄ± Ã§alÄ±ÅŸtÄ±r
    test_classes = [
        TestResponseTimeBenchmarks(),
        TestLoadAndStressTesting(),
        TestResourceUsageMonitoring()
    ]
    
    for test_class in test_classes:
        class_name = test_class.__class__.__name__
        print(f"\nğŸ“‹ {class_name} testleri Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        # Test metodlarÄ±nÄ± bul ve Ã§alÄ±ÅŸtÄ±r
        test_methods = [method for method in dir(test_class) if method.startswith('test_')]
        
        for method_name in test_methods:
            test_results['total_tests'] += 1
            
            try:
                # Test kurulumunu Ã§alÄ±ÅŸtÄ±r
                if hasattr(test_class, 'setup'):
                    test_class.setup()
                
                # Test metodunu Ã§alÄ±ÅŸtÄ±r
                method = getattr(test_class, method_name)
                result = method()
                
                test_results['passed_tests'] += 1
                test_results['test_details'].append({
                    'test': f"{class_name}.{method_name}",
                    'status': 'PASSED',
                    'metrics': result if isinstance(result, dict) else {}
                })
                
                # Metrikleri topla
                if isinstance(result, dict):
                    test_results['performance_metrics'][f"{class_name}.{method_name}"] = result
                
            except Exception as e:
                test_results['failed_tests'] += 1
                test_results['test_details'].append({
                    'test': f"{class_name}.{method_name}",
                    'status': 'FAILED',
                    'error': str(e)
                })
                print(f"   âŒ Test failed: {method_name} - {str(e)}")
    
    # SonuÃ§larÄ± Ã¶zetle
    print(f"\nğŸ“Š PERFORMANS TEST SONUÃ‡LARI")
    print("=" * 60)
    print(f"Toplam Test: {test_results['total_tests']}")
    print(f"BaÅŸarÄ±lÄ±: {test_results['passed_tests']}")
    print(f"BaÅŸarÄ±sÄ±z: {test_results['failed_tests']}")
    print(f"BaÅŸarÄ± OranÄ±: %{(test_results['passed_tests']/test_results['total_tests']*100):.1f}")
    
    # Performans Ã¶zeti
    if test_results['performance_metrics']:
        print(f"\nğŸ’¡ PERFORMANS Ã–ZETÄ°:")
        print("-" * 40)
        
        # Response time Ã¶zeti
        response_time_tests = [k for k in test_results['performance_metrics'].keys() if 'response' in k.lower()]
        if response_time_tests:
            print("âš¡ YanÄ±t SÃ¼resi PerformansÄ±: Ä°yi")
        
        # Load test Ã¶zeti  
        load_tests = [k for k in test_results['performance_metrics'].keys() if 'load' in k.lower() or 'stress' in k.lower()]
        if load_tests:
            print("ğŸ‹ï¸ YÃ¼k Testi PerformansÄ±: Stabil")
        
        # Resource usage Ã¶zeti
        resource_tests = [k for k in test_results['performance_metrics'].keys() if 'resource' in k.lower()]
        if resource_tests:
            print("ğŸ“Š Kaynak KullanÄ±mÄ±: Optimize")
    
    return test_results

if __name__ == "__main__":
    # DoÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda testleri baÅŸlat
    results = run_performance_tests()
    
    # Ã‡Ä±kÄ±ÅŸ kodu belirle
    if results['failed_tests'] == 0:
        exit_code = 0
    else:
        exit_code = 1
    
    exit(exit_code)
