# ‚ö° AURA AI Sƒ∞STEMƒ∞ - PERFORMANS TEST PAKETƒ∞
# Test Odaklƒ± Geri Besleme D√∂ng√ºs√º (AlphaCodium/SED) Performans Benchmark Sistemi

import pytest
import time
import asyncio
import statistics
import threading
import concurrent.futures
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
import json
import psutil
import requests
from unittest.mock import Mock, patch

# Test framework konfig√ºrasyonunu import et
from ..conftest import AuraTestConfig, TestUtilities

class PerformanceTestSuite:
    """
    Performans test paketi.
    
    Bu sƒ±nƒ±f, sistemin √ße≈üitli y√ºk ko≈üullarƒ±ndaki performansƒ±nƒ± √∂l√ßer.
    Yanƒ±t s√ºreleri, throughput, kaynak kullanƒ±mƒ± ve scalability testleri yapar.
    """
    
    def __init__(self):
        # Test konfig√ºrasyonunu y√ºkle
        self.config = AuraTestConfig()
        self.utils = TestUtilities()
        
        # Performans kriterleri ve thresholdlar
        self.performance_thresholds = {
            'response_time_ms': {
                'excellent': 200,
                'good': 500,
                'acceptable': 1000,
                'poor': 2000,
                'unacceptable': 5000
            },
            'throughput_rps': {
                'minimum': 10,
                'target': 50,  
                'excellent': 100
            },
            'concurrent_users': {
                'minimum': 10,
                'target': 50,
                'maximum': 200
            },
            'resource_usage': {
                'cpu_percent': 80,
                'memory_mb': 1024,
                'disk_io_mbps': 100
            }
        }
        
        # Test sonu√ßlarƒ±nƒ± saklamak i√ßin
        self.performance_results = {
            'response_time_tests': {},
            'load_tests': {},
            'stress_tests': {},
            'endurance_tests': {},
            'resource_usage_tests': {}
        }

# Test sƒ±nƒ±flarƒ±
class TestResponseTimeBenchmarks:
    """
    Yanƒ±t s√ºresi benchmark testleri.
    
    Bu test grubu, her servisin farklƒ± y√ºk ko≈üullarƒ±ndaki yanƒ±t s√ºrelerini √∂l√ßer.
    """
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """Her test √∂ncesi gerekli kurulumlarƒ± yap"""
        self.config = AuraTestConfig()
        self.utils = TestUtilities()
        
        print("\n‚ö° Yanƒ±t s√ºresi benchmark testleri ba≈ülatƒ±lƒ±yor...")
    
    def test_single_request_response_times(self):
        """
        Tek istek yanƒ±t s√ºrelerini test et.
        
        Her servise tek istek g√∂nderip yanƒ±t s√ºrelerini √∂l√ß.
        Bu baseline performans metriƒüidir.
        """
        print("   üîç Test: Tek istek yanƒ±t s√ºreleri")
        
        # Her servis i√ßin yanƒ±t s√ºresi √∂l√ß
        response_times = {}
        
        for service_name, service_url in self.config.SERVICES.items():
            print(f"      üìä {service_name} yanƒ±t s√ºresi √∂l√ß√ºl√ºyor...")
            
            # 10 kez √∂l√ß√ºm yapƒ±p ortalama al
            measurements = []
            
            for i in range(10):
                start_time = time.time()
                
                try:
                    # Mock request sim√ºlasyonu
                    response = self._simulate_service_request(service_name, service_url)
                    response_time_ms = (time.time() - start_time) * 1000
                    measurements.append(response_time_ms)
                    
                except Exception as e:
                    print(f"         ‚ö†Ô∏è √ñl√ß√ºm {i+1} ba≈üarƒ±sƒ±z: {str(e)}")
                    measurements.append(5000)  # Timeout deƒüeri
            
            # ƒ∞statistikleri hesapla
            response_times[service_name] = {
                'avg_ms': statistics.mean(measurements),
                'min_ms': min(measurements),
                'max_ms': max(measurements),
                'median_ms': statistics.median(measurements),
                'std_dev': statistics.stdev(measurements) if len(measurements) > 1 else 0,
                'measurements': measurements
            }
            
            avg_time = response_times[service_name]['avg_ms']
            performance_level = self._categorize_response_time(avg_time)
            
            print(f"         ‚è±Ô∏è Ortalama: {avg_time:.2f}ms ({performance_level})")
            print(f"         üìà Min: {response_times[service_name]['min_ms']:.2f}ms")
            print(f"         üìâ Max: {response_times[service_name]['max_ms']:.2f}ms")
        
        # Genel deƒüerlendirme
        total_avg = statistics.mean([rt['avg_ms'] for rt in response_times.values()])
        overall_performance = self._categorize_response_time(total_avg)
        
        print(f"\n      üéØ Genel ortalama yanƒ±t s√ºresi: {total_avg:.2f}ms ({overall_performance})")
        
        # Doƒürulamalar
        slow_services = [name for name, data in response_times.items() 
                        if data['avg_ms'] > self.config.PERFORMANCE_THRESHOLDS['response_time_ms']]
        
        assert len(slow_services) == 0, f"Yava≈ü servisler tespit edildi: {slow_services}"
        assert total_avg < 1000, f"Genel ortalama yanƒ±t s√ºresi √ßok y√ºksek: {total_avg:.2f}ms"
        
        return response_times
    
    def test_concurrent_request_response_times(self):
        """
        E≈üzamanlƒ± istek yanƒ±t s√ºrelerini test et.
        
        Aynƒ± anda birden fazla istek g√∂nderip yanƒ±t s√ºrelerinin nasƒ±l etkilendiƒüini √∂l√ß.
        """
        print("   üîç Test: E≈üzamanlƒ± istek yanƒ±t s√ºreleri")
        
        concurrent_levels = [5, 10, 20, 50]  # E≈üzamanlƒ±lƒ±k seviyeleri
        concurrent_results = {}
        
        for concurrent_count in concurrent_levels:
            print(f"      üìä {concurrent_count} e≈üzamanlƒ± istek testi...")
            
            # E≈üzamanlƒ± istekler i√ßin thread pool
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_count) as executor:
                start_time = time.time()
                
                # T√ºm istekleri ba≈ülat
                futures = []
                for i in range(concurrent_count):
                    service_name = list(self.config.SERVICES.keys())[i % len(self.config.SERVICES)]
                    service_url = self.config.SERVICES[service_name]
                    
                    future = executor.submit(self._simulate_service_request, service_name, service_url)
                    futures.append((future, service_name))
                
                # Sonu√ßlarƒ± topla
                results = []
                for future, service_name in futures:
                    try:
                        future.result(timeout=10)  # 10 saniye timeout
                        request_time = time.time() - start_time
                        results.append({'service': service_name, 'time_ms': request_time * 1000})
                    except Exception as e:
                        results.append({'service': service_name, 'time_ms': 10000, 'error': str(e)})
                
                total_time = time.time() - start_time
                
                # ƒ∞statistikleri hesapla
                successful_requests = [r for r in results if 'error' not in r]
                failed_requests = [r for r in results if 'error' in r]
                
                concurrent_results[concurrent_count] = {
                    'total_time_ms': total_time * 1000,
                    'successful_count': len(successful_requests),
                    'failed_count': len(failed_requests),
                    'success_rate': len(successful_requests) / concurrent_count * 100,
                    'avg_response_time_ms': statistics.mean([r['time_ms'] for r in successful_requests]) if successful_requests else 0,
                    'throughput_rps': concurrent_count / total_time if total_time > 0 else 0
                }
                
                result = concurrent_results[concurrent_count]
                print(f"         ‚úÖ Ba≈üarƒ±lƒ±: {result['successful_count']}/{concurrent_count}")
                print(f"         ‚è±Ô∏è Ortalama yanƒ±t: {result['avg_response_time_ms']:.2f}ms")
                print(f"         üöÄ Throughput: {result['throughput_rps']:.2f} req/sec")
                print(f"         üìä Ba≈üarƒ± oranƒ±: %{result['success_rate']:.1f}")
        
        # Performans degradasyonunu analiz et
        baseline_throughput = concurrent_results[5]['throughput_rps'] if 5 in concurrent_results else 0
        
        for level, result in concurrent_results.items():
            if level > 5:
                degradation = ((baseline_throughput - result['throughput_rps']) / baseline_throughput) * 100 if baseline_throughput > 0 else 0
                print(f"      üìâ {level} e≈üzamanlƒ± istek degradasyonu: %{degradation:.1f}")
        
        # Doƒürulamalar
        for level, result in concurrent_results.items():
            assert result['success_rate'] >= 90, f"{level} e≈üzamanlƒ± istekte ba≈üarƒ± oranƒ± d√º≈ü√ºk: %{result['success_rate']:.1f}"
            assert result['throughput_rps'] >= 5, f"{level} e≈üzamanlƒ± istekte throughput √ßok d√º≈ü√ºk: {result['throughput_rps']:.2f}"
        
        return concurrent_results
    
    def _simulate_service_request(self, service_name: str, service_url: str) -> Dict:
        """Servis isteƒüini sim√ºle et"""
        # Mock response sim√ºlasyonu
        simulation_delay = {
            'image_processing_service': 0.15,  # 150ms
            'nlu_service': 0.12,              # 120ms
            'style_profile_service': 0.08,    # 80ms
            'combination_engine_service': 0.25, # 250ms
            'recommendation_engine_service': 0.18, # 180ms
            'orchestrator_service': 0.3,      # 300ms
            'feedback_loop_service': 0.1      # 100ms
        }
        
        # Ger√ßek√ßi gecikme sim√ºlasyonu
        delay = simulation_delay.get(service_name, 0.1)
        time.sleep(delay)
        
        return {
            "status": "success",
            "service": service_name,
            "timestamp": datetime.now().isoformat(),
            "simulated": True
        }
    
    def _categorize_response_time(self, response_time_ms: float) -> str:
        """Yanƒ±t s√ºresini kategorize et"""
        thresholds = {
            200: "m√ºkemmel",
            500: "iyi", 
            1000: "kabul edilebilir",
            2000: "zayƒ±f",
            float('inf'): "kabul edilemez"
        }
        
        for threshold, category in thresholds.items():
            if response_time_ms <= threshold:
                return category
        
        return "bilinmeyen"

class TestLoadAndStressTesting:
    """
    Y√ºk ve stres testleri.
    
    Bu test grubu, sistemin y√ºksek y√ºk altƒ±ndaki davranƒ±≈üƒ±nƒ± test eder.
    """
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """Her test √∂ncesi gerekli kurulumlarƒ± yap"""
        self.config = AuraTestConfig()
        self.utils = TestUtilities()
        
        print("\nüèãÔ∏è Y√ºk ve stres testleri ba≈ülatƒ±lƒ±yor...")
    
    def test_load_testing_sustained_traffic(self):
        """
        S√ºrekli trafik y√ºk testi.
        
        5 dakika boyunca s√ºrekli istekler g√∂nder ve sistemin stabilitesini √∂l√ß.
        """
        print("   üîç Test: S√ºrekli trafik y√ºk testi (5 dakika)")
        
        test_duration_seconds = 30  # Test i√ßin kƒ±saltƒ±lmƒ±≈ü s√ºre (ger√ßekte 300 olacak)
        requests_per_second = 10
        
        start_time = time.time()
        end_time = start_time + test_duration_seconds
        
        results = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'response_times': [],
            'error_details': []
        }
        
        print(f"      ‚è±Ô∏è {test_duration_seconds} saniye boyunca {requests_per_second} req/sec g√∂nderiliyor...")
        
        # Load test d√∂ng√ºs√º
        while time.time() < end_time:
            cycle_start = time.time()
            
            # Bu saniyede g√∂nderilecek istekler
            for i in range(requests_per_second):
                request_start = time.time()
                
                try:
                    # Random servis se√ß
                    service_name = list(self.config.SERVICES.keys())[i % len(self.config.SERVICES)]
                    service_url = self.config.SERVICES[service_name]
                    
                    # ƒ∞stek sim√ºle et
                    response = self._simulate_service_request(service_name, service_url)
                    response_time = (time.time() - request_start) * 1000
                    
                    results['total_requests'] += 1
                    results['successful_requests'] += 1
                    results['response_times'].append(response_time)
                    
                except Exception as e:
                    results['total_requests'] += 1
                    results['failed_requests'] += 1
                    results['error_details'].append(str(e))
                
                # Rate limiting (saniyede belirlenen sayƒ±da istek)
                if i < requests_per_second - 1:
                    time.sleep(1.0 / requests_per_second)
            
            # Bir sonraki saniyeye kadar bekle
            cycle_time = time.time() - cycle_start
            if cycle_time < 1.0:
                time.sleep(1.0 - cycle_time)
        
        # Sonu√ßlarƒ± analiz et
        actual_duration = time.time() - start_time
        actual_rps = results['total_requests'] / actual_duration
        success_rate = (results['successful_requests'] / results['total_requests']) * 100 if results['total_requests'] > 0 else 0
        
        avg_response_time = statistics.mean(results['response_times']) if results['response_times'] else 0
        p95_response_time = sorted(results['response_times'])[int(len(results['response_times']) * 0.95)] if results['response_times'] else 0
        
        print(f"      üìä Test sonu√ßlarƒ±:")
        print(f"         Toplam istek: {results['total_requests']}")
        print(f"         Ba≈üarƒ±lƒ±: {results['successful_requests']} (%{success_rate:.1f})")
        print(f"         Ba≈üarƒ±sƒ±z: {results['failed_requests']}")
        print(f"         Ger√ßek RPS: {actual_rps:.2f}")
        print(f"         Ortalama yanƒ±t: {avg_response_time:.2f}ms")
        print(f"         P95 yanƒ±t: {p95_response_time:.2f}ms")
        
        # Doƒürulamalar
        assert success_rate >= 95, f"Ba≈üarƒ± oranƒ± √ßok d√º≈ü√ºk: %{success_rate:.1f}"
        assert actual_rps >= requests_per_second * 0.8, f"Throughput target'ƒ±n altƒ±nda: {actual_rps:.2f}"
        assert avg_response_time < 1000, f"Ortalama yanƒ±t s√ºresi √ßok y√ºksek: {avg_response_time:.2f}ms"
        assert p95_response_time < 2000, f"P95 yanƒ±t s√ºresi √ßok y√ºksek: {p95_response_time:.2f}ms"
        
        return results
    
    def test_stress_testing_breaking_point(self):
        """
        Stres testi - kƒ±rƒ±lma noktasƒ± bulma.
        
        Gittik√ße artan y√ºkle sistemin kƒ±rƒ±lma noktasƒ±nƒ± bul.
        """
        print("   üîç Test: Stres testi - kƒ±rƒ±lma noktasƒ±")
        
        stress_levels = [10, 25, 50, 75, 100, 150, 200]  # RPS deƒüerleri
        stress_results = {}
        breaking_point = None
        
        for rps_level in stress_levels:
            print(f"      üìà {rps_level} RPS stres seviyesi test ediliyor...")
            
            test_duration = 10  # Her seviye i√ßin 10 saniye
            start_time = time.time()
            
            level_results = {
                'target_rps': rps_level,
                'total_requests': 0,
                'successful_requests': 0,
                'failed_requests': 0,
                'response_times': [],
                'errors': []
            }
            
            # Stress test d√∂ng√ºs√º
            with concurrent.futures.ThreadPoolExecutor(max_workers=min(rps_level, 50)) as executor:
                futures = []
                
                # Belirtilen RPS'de istekler g√∂nder
                for second in range(test_duration):
                    for request_num in range(rps_level):
                        service_name = list(self.config.SERVICES.keys())[request_num % len(self.config.SERVICES)]
                        service_url = self.config.SERVICES[service_name]
                        
                        future = executor.submit(self._simulate_service_request, service_name, service_url)
                        futures.append(future)
                        level_results['total_requests'] += 1
                    
                    # Rate limiting
                    time.sleep(0.1)  # Kƒ±sa sleep interval
                
                # T√ºm sonu√ßlarƒ± topla
                for future in concurrent.futures.as_completed(futures, timeout=30):
                    try:
                        result = future.result(timeout=5)
                        level_results['successful_requests'] += 1
                    except Exception as e:
                        level_results['failed_requests'] += 1
                        level_results['errors'].append(str(e))
            
            # Seviye sonu√ßlarƒ±nƒ± hesapla
            actual_duration = time.time() - start_time
            actual_rps = level_results['total_requests'] / actual_duration
            success_rate = (level_results['successful_requests'] / level_results['total_requests']) * 100 if level_results['total_requests'] > 0 else 0
            
            stress_results[rps_level] = {
                'actual_rps': actual_rps,
                'success_rate': success_rate,
                'failed_requests': level_results['failed_requests'],
                'error_count': len(level_results['errors'])
            }
            
            print(f"         ‚ö° Ger√ßek RPS: {actual_rps:.2f}")
            print(f"         ‚úÖ Ba≈üarƒ± oranƒ±: %{success_rate:.1f}")
            print(f"         ‚ùå Hata sayƒ±sƒ±: {level_results['failed_requests']}")
            
            # Kƒ±rƒ±lma noktasƒ±nƒ± belirle
            if success_rate < 90 or level_results['failed_requests'] > level_results['total_requests'] * 0.1:
                breaking_point = rps_level
                print(f"         üî• Kƒ±rƒ±lma noktasƒ± tespit edildi: {rps_level} RPS")
                break
            
            # Rate limiting - bir sonraki seviye i√ßin bekle
            time.sleep(2)
        
        # Sonu√ßlarƒ± √∂zetle
        if breaking_point:
            print(f"\n      üéØ Sistem kƒ±rƒ±lma noktasƒ±: {breaking_point} RPS")
            recommended_max = int(breaking_point * 0.7)  # %70'i g√ºvenli kabul et
            print(f"      üí° √ñnerilen maksimum y√ºk: {recommended_max} RPS")
        else:
            print(f"\n      üöÄ Sistem {max(stress_levels)} RPS'e kadar stabil!")
        
        # Doƒürulamalar
        assert len(stress_results) > 0, "Hi√ß stress testi tamamlanamadƒ±"
        
        # En az 50 RPS'i kaldƒ±rabilmeli
        stable_levels = [level for level, result in stress_results.items() if result['success_rate'] >= 90]
        assert len(stable_levels) > 0 and max(stable_levels) >= 50, f"Sistem 50 RPS'i kaldƒ±ramƒ±yor: {stable_levels}"
        
        return stress_results
    
    def _simulate_service_request(self, service_name: str, service_url: str) -> Dict:
        """Servis isteƒüini sim√ºle et (load test i√ßin optimized)"""
        # Daha hƒ±zlƒ± sim√ºlasyon i√ßin kƒ±saltƒ±lmƒ±≈ü gecikmeler
        simulation_delay = {
            'image_processing_service': 0.05,   # 50ms
            'nlu_service': 0.03,               # 30ms
            'style_profile_service': 0.02,     # 20ms
            'combination_engine_service': 0.08, # 80ms
            'recommendation_engine_service': 0.06, # 60ms
            'orchestrator_service': 0.1,       # 100ms
            'feedback_loop_service': 0.03      # 30ms
        }
        
        # Ger√ßek√ßi gecikme sim√ºlasyonu
        delay = simulation_delay.get(service_name, 0.03)
        time.sleep(delay)
        
        return {
            "status": "success",
            "service": service_name,
            "timestamp": datetime.now().isoformat(),
            "simulated": True
        }

class TestResourceUsageMonitoring:
    """
    Kaynak kullanƒ±mƒ± izleme testleri.
    
    Bu test grubu, sistem kaynaklarƒ±nƒ±n (CPU, RAM, Disk) kullanƒ±mƒ±nƒ± izler.
    """
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """Her test √∂ncesi gerekli kurulumlarƒ± yap"""
        self.config = AuraTestConfig()
        self.utils = TestUtilities()
        
        print("\nüìä Kaynak kullanƒ±mƒ± izleme testleri ba≈ülatƒ±lƒ±yor...")
    
    def test_resource_usage_under_normal_load(self):
        """
        Normal y√ºk altƒ±nda kaynak kullanƒ±mƒ±nƒ± test et.
        
        Normal i≈ülem y√ºk√º altƒ±nda CPU, RAM ve disk kullanƒ±mƒ±nƒ± izle.
        """
        print("   üîç Test: Normal y√ºk altƒ±nda kaynak kullanƒ±mƒ±")
        
        # Baseline kaynak kullanƒ±mƒ±nƒ± √∂l√ß
        baseline_stats = self._get_system_stats()
        print(f"      üìà Baseline CPU: %{baseline_stats['cpu_percent']:.1f}")
        print(f"      üß† Baseline RAM: {baseline_stats['memory_mb']:.1f}MB")
        print(f"      üíæ Baseline Disk: {baseline_stats['disk_io_mbps']:.2f}MB/s")
        
        # Normal y√ºk sim√ºlasyonu (30 saniye)
        test_duration = 10  # Test i√ßin kƒ±saltƒ±lmƒ±≈ü
        monitoring_interval = 1  # Her 1 saniyede √∂l√ß√ºm
        
        resource_measurements = []
        
        print(f"      ‚è±Ô∏è {test_duration} saniye normal y√ºk sim√ºlasyonu...")
        
        start_time = time.time()
        end_time = start_time + test_duration
        
        # Normal y√ºk thread'i ba≈ülat
        load_thread = threading.Thread(target=self._simulate_normal_load, args=(test_duration,))
        load_thread.start()
        
        # Kaynak kullanƒ±mƒ±nƒ± izle
        while time.time() < end_time:
            current_stats = self._get_system_stats()
            resource_measurements.append({
                'timestamp': time.time() - start_time,
                'cpu_percent': current_stats['cpu_percent'],
                'memory_mb': current_stats['memory_mb'],
                'disk_io_mbps': current_stats['disk_io_mbps']
            })
            
            time.sleep(monitoring_interval)
        
        # Load thread'in bitmesini bekle
        load_thread.join()
        
        # ƒ∞statistikleri hesapla
        if resource_measurements:
            avg_cpu = statistics.mean([m['cpu_percent'] for m in resource_measurements])
            max_cpu = max([m['cpu_percent'] for m in resource_measurements])
            avg_memory = statistics.mean([m['memory_mb'] for m in resource_measurements])
            max_memory = max([m['memory_mb'] for m in resource_measurements])
            avg_disk = statistics.mean([m['disk_io_mbps'] for m in resource_measurements])
            max_disk = max([m['disk_io_mbps'] for m in resource_measurements])
            
            print(f"      üìä Test sonu√ßlarƒ±:")
            print(f"         Ortalama CPU: %{avg_cpu:.1f} (Max: %{max_cpu:.1f})")
            print(f"         Ortalama RAM: {avg_memory:.1f}MB (Max: {max_memory:.1f}MB)")
            print(f"         Ortalama Disk I/O: {avg_disk:.2f}MB/s (Max: {max_disk:.2f}MB/s)")
            
            # Performans deƒüerlendirmesi
            cpu_status = "iyi" if avg_cpu < 50 else "orta" if avg_cpu < 80 else "y√ºksek"
            memory_status = "iyi" if avg_memory < 512 else "orta" if avg_memory < 1024 else "y√ºksek"
            
            print(f"         üéØ CPU kullanƒ±mƒ±: {cpu_status}")
            print(f"         üéØ RAM kullanƒ±mƒ±: {memory_status}")
        
            # Doƒürulamalar
            assert avg_cpu < 80, f"Ortalama CPU kullanƒ±mƒ± √ßok y√ºksek: %{avg_cpu:.1f}"
            assert max_cpu < 95, f"Maksimum CPU kullanƒ±mƒ± √ßok y√ºksek: %{max_cpu:.1f}"
            assert avg_memory < 1024, f"Ortalama RAM kullanƒ±mƒ± √ßok y√ºksek: {avg_memory:.1f}MB"
            
            return {
                'avg_cpu': avg_cpu,
                'max_cpu': max_cpu,
                'avg_memory': avg_memory,
                'max_memory': max_memory,
                'measurements': resource_measurements
            }
        else:
            print("      ‚ö†Ô∏è Kaynak √∂l√ß√ºm√º alƒ±namadƒ±")
            return {}
    
    def test_resource_usage_under_heavy_load(self):
        """
        Yoƒüun y√ºk altƒ±nda kaynak kullanƒ±mƒ±nƒ± test et.
        
        Y√ºksek i≈ülem y√ºk√º altƒ±nda kaynak kullanƒ±mƒ±nƒ± izle ve limitleri kontrol et.
        """
        print("   üîç Test: Yoƒüun y√ºk altƒ±nda kaynak kullanƒ±mƒ±")
        
        test_duration = 15  # Test i√ßin kƒ±saltƒ±lmƒ±≈ü
        monitoring_interval = 0.5  # Daha sƒ±k √∂l√ß√ºm
        
        resource_measurements = []
        
        print(f"      ‚ö° {test_duration} saniye yoƒüun y√ºk sim√ºlasyonu...")
        
        start_time = time.time()
        end_time = start_time + test_duration
        
        # Yoƒüun y√ºk thread'leri ba≈ülat
        heavy_load_threads = []
        for i in range(3):  # 3 parallel heavy load thread
            thread = threading.Thread(target=self._simulate_heavy_load, args=(test_duration,))
            thread.start()
            heavy_load_threads.append(thread)
        
        # Kaynak kullanƒ±mƒ±nƒ± izle
        while time.time() < end_time:
            current_stats = self._get_system_stats()
            resource_measurements.append({
                'timestamp': time.time() - start_time,
                'cpu_percent': current_stats['cpu_percent'],
                'memory_mb': current_stats['memory_mb'],
                'disk_io_mbps': current_stats['disk_io_mbps']
            })
            
            time.sleep(monitoring_interval)
        
        # T√ºm thread'lerin bitmesini bekle
        for thread in heavy_load_threads:
            thread.join()
        
        # ƒ∞statistikleri hesapla
        if resource_measurements:
            avg_cpu = statistics.mean([m['cpu_percent'] for m in resource_measurements])
            max_cpu = max([m['cpu_percent'] for m in resource_measurements])
            avg_memory = statistics.mean([m['memory_mb'] for m in resource_measurements])
            max_memory = max([m['memory_mb'] for m in resource_measurements])
            
            print(f"      üìä Yoƒüun y√ºk test sonu√ßlarƒ±:")
            print(f"         Ortalama CPU: %{avg_cpu:.1f} (Max: %{max_cpu:.1f})")
            print(f"         Ortalama RAM: {avg_memory:.1f}MB (Max: {max_memory:.1f}MB)")
            
            # Kritik seviye uyarƒ±larƒ±
            if max_cpu > 90:
                print(f"         üî• UYARI: CPU kullanƒ±mƒ± kritik seviyede! %{max_cpu:.1f}")
            if max_memory > 2048:
                print(f"         üî• UYARI: RAM kullanƒ±mƒ± kritik seviyede! {max_memory:.1f}MB")
            
            # Doƒürulamalar (heavy load i√ßin daha y√ºksek thresholdlar)
            assert max_cpu < 98, f"CPU kullanƒ±mƒ± sistem limitini a≈ütƒ±: %{max_cpu:.1f}"
            assert max_memory < 4096, f"RAM kullanƒ±mƒ± sistem limitini a≈ütƒ±: {max_memory:.1f}MB"
            
            return {
                'avg_cpu': avg_cpu,
                'max_cpu': max_cpu,
                'avg_memory': avg_memory,
                'max_memory': max_memory,
                'peak_usage_detected': max_cpu > 85 or max_memory > 1536
            }
        else:
            print("      ‚ö†Ô∏è Kaynak √∂l√ß√ºm√º alƒ±namadƒ±")
            return {}
    
    def _get_system_stats(self) -> Dict:
        """Mevcut sistem istatistiklerini al"""
        try:
            # psutil ile ger√ßek sistem istatistikleri
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory_info = psutil.virtual_memory()
            memory_mb = memory_info.used / (1024 * 1024)
            
            # Disk I/O (basit sim√ºlasyon)
            disk_io = psutil.disk_io_counters()
            disk_io_mbps = (disk_io.read_bytes + disk_io.write_bytes) / (1024 * 1024) if disk_io else 0
            
            return {
                'cpu_percent': cpu_percent,
                'memory_mb': memory_mb,
                'disk_io_mbps': disk_io_mbps,
                'timestamp': time.time()
            }
        except Exception as e:
            # Fallback mock data
            return {
                'cpu_percent': 25.0,
                'memory_mb': 256.0,
                'disk_io_mbps': 5.0,
                'timestamp': time.time(),
                'mock': True
            }
    
    def _simulate_normal_load(self, duration: int):
        """Normal y√ºk sim√ºlasyonu"""
        end_time = time.time() + duration
        
        while time.time() < end_time:
            # Basit CPU i≈ülemleri
            for i in range(1000):
                _ = i ** 2
            
            # Kƒ±sa memory allocation
            temp_data = [i for i in range(1000)]
            del temp_data
            
            time.sleep(0.01)  # CPU'yu tamamen me≈ügul etme
    
    def _simulate_heavy_load(self, duration: int):
        """Yoƒüun y√ºk sim√ºlasyonu"""
        end_time = time.time() + duration
        
        while time.time() < end_time:
            # Yoƒüun CPU i≈ülemleri
            for i in range(10000):
                _ = i ** 3
            
            # Memory allocation
            temp_data = [i for i in range(5000)]
            del temp_data
            
            time.sleep(0.001)  # Minimal sleep

# Ana test fonksiyonu
def run_performance_tests():
    """
    T√ºm performans testlerini √ßalƒ±≈ütƒ±r.
    
    Bu fonksiyon, sistemin performans karakteristiklerini kapsamlƒ± bir ≈üekilde test eder.
    """
    print("‚ö° PERFORMANS TEST PAKETƒ∞ BA≈ûLATILIYOR")
    print("=" * 60)
    
    test_results = {
        'total_tests': 0,
        'passed_tests': 0,
        'failed_tests': 0,
        'performance_metrics': {},
        'test_details': []
    }
    
    # T√ºm test sƒ±nƒ±flarƒ±nƒ± √ßalƒ±≈ütƒ±r
    test_classes = [
        TestResponseTimeBenchmarks(),
        TestLoadAndStressTesting(),
        TestResourceUsageMonitoring()
    ]
    
    for test_class in test_classes:
        class_name = test_class.__class__.__name__
        print(f"\nüìã {class_name} testleri √ßalƒ±≈ütƒ±rƒ±lƒ±yor...")
        
        # Test metodlarƒ±nƒ± bul ve √ßalƒ±≈ütƒ±r
        test_methods = [method for method in dir(test_class) if method.startswith('test_')]
        
        for method_name in test_methods:
            test_results['total_tests'] += 1
            
            try:
                # Test kurulumunu √ßalƒ±≈ütƒ±r
                if hasattr(test_class, 'setup'):
                    test_class.setup()
                
                # Test metodunu √ßalƒ±≈ütƒ±r
                method = getattr(test_class, method_name)
                result = method()
                
                test_results['passed_tests'] += 1
                test_results['test_details'].append({
                    'test': f"{class_name}.{method_name}",
                    'status': 'PASSED',
                    'metrics': result if isinstance(result, dict) else {}
                })
                
                # Metrikleri topla
                if isinstance(result, dict):
                    test_results['performance_metrics'][f"{class_name}.{method_name}"] = result
                
            except Exception as e:
                test_results['failed_tests'] += 1
                test_results['test_details'].append({
                    'test': f"{class_name}.{method_name}",
                    'status': 'FAILED',
                    'error': str(e)
                })
                print(f"   ‚ùå Test failed: {method_name} - {str(e)}")
    
    # Sonu√ßlarƒ± √∂zetle
    print(f"\nüìä PERFORMANS TEST SONU√áLARI")
    print("=" * 60)
    print(f"Toplam Test: {test_results['total_tests']}")
    print(f"Ba≈üarƒ±lƒ±: {test_results['passed_tests']}")
    print(f"Ba≈üarƒ±sƒ±z: {test_results['failed_tests']}")
    print(f"Ba≈üarƒ± Oranƒ±: %{(test_results['passed_tests']/test_results['total_tests']*100):.1f}")
    
    # Performans √∂zeti
    if test_results['performance_metrics']:
        print(f"\nüí° PERFORMANS √ñZETƒ∞:")
        print("-" * 40)
        
        # Response time √∂zeti
        response_time_tests = [k for k in test_results['performance_metrics'].keys() if 'response' in k.lower()]
        if response_time_tests:
            print("‚ö° Yanƒ±t S√ºresi Performansƒ±: ƒ∞yi")
        
        # Load test √∂zeti  
        load_tests = [k for k in test_results['performance_metrics'].keys() if 'load' in k.lower() or 'stress' in k.lower()]
        if load_tests:
            print("üèãÔ∏è Y√ºk Testi Performansƒ±: Stabil")
        
        # Resource usage √∂zeti
        resource_tests = [k for k in test_results['performance_metrics'].keys() if 'resource' in k.lower()]
        if resource_tests:
            print("üìä Kaynak Kullanƒ±mƒ±: Optimize")
    
    return test_results

if __name__ == "__main__":
    # Doƒürudan √ßalƒ±≈ütƒ±rƒ±ldƒ±ƒüƒ±nda testleri ba≈ülat
    results = run_performance_tests()
    
    # √áƒ±kƒ±≈ü kodu belirle
    if results['failed_tests'] == 0:
        exit_code = 0
    else:
        exit_code = 1
    
    exit(exit_code)
