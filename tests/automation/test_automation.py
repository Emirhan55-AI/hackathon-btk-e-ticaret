# ğŸ¤– AURA AI SÄ°STEMÄ° - OTOMATÄ°K TEST VE RAPOR SÄ°STEMÄ°
# Test OdaklÄ± Geri Besleme DÃ¶ngÃ¼sÃ¼ (AlphaCodium/SED) Otomasyon Platformu

import os
import sys
import time
import json
import subprocess
import schedule
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders

class AuraTestAutomation:
    """
    Aura AI sistemi iÃ§in otomatik test ve raporlama platformu.
    
    Bu sÄ±nÄ±f, testlerin otomatik Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ±, raporlarÄ±n oluÅŸturulmasÄ±
    ve sonuÃ§larÄ±n ilgili kiÅŸilere iletilmesi iÅŸlemlerini yÃ¶netir.
    """
    
    def __init__(self, config_file: Optional[str] = None):
        # KonfigÃ¼rasyon dosyasÄ±nÄ± yÃ¼kle
        self.config = self._load_configuration(config_file)
        
        # Test sonuÃ§larÄ± ve raporlar iÃ§in dizinler
        self.base_dir = Path(__file__).parent.parent
        self.reports_dir = self.base_dir / "tests" / "reports"
        self.archive_dir = self.reports_dir / "archive"
        self.logs_dir = self.base_dir / "logs"
        
        # Dizinleri oluÅŸtur
        self._ensure_directories()
        
        # Test scheduling durumu
        self.is_scheduled = False
        self.current_schedule = {}
        
        print("ğŸ¤– AURA AI OTOMATÄ°K TEST SÄ°STEMÄ°")
        print("=" * 50)
        print("ğŸ“… Sistem baÅŸlatÄ±ldÄ±:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        print("ğŸ“ Rapor dizini:", self.reports_dir)
        print("ğŸ“Š Test otomasyonu hazÄ±r!")
        print("=" * 50)
    
    def _load_configuration(self, config_file: Optional[str]) -> Dict:
        """Test otomasyon konfigÃ¼rasyonunu yÃ¼kle"""
        default_config = {
            "test_schedule": {
                "daily_full_tests": "02:00",  # Her gÃ¼n saat 02:00'da tam test
                "hourly_health_checks": True,  # Her saatte saÄŸlÄ±k kontrolÃ¼
                "weekly_performance_tests": "Sunday 01:00",  # HaftalÄ±k performans testi
                "continuous_monitoring": True  # SÃ¼rekli izleme aktif
            },
            "test_types": {
                "unit_tests": True,
                "integration_tests": True,
                "e2e_tests": True,
                "fault_tolerance_tests": True,
                "performance_tests": True
            },
            "reporting": {
                "html_reports": True,
                "json_reports": True,
                "email_notifications": False,  # E-posta bildirimleri kapalÄ± (demo iÃ§in)
                "slack_notifications": False,  # Slack bildirimleri kapalÄ±
                "dashboard_updates": True
            },
            "thresholds": {
                "max_failure_rate": 5,  # %5'den fazla baÅŸarÄ±sÄ±zlÄ±k varsa alarm
                "max_response_time_ms": 2000,  # 2 saniyeden uzun yanÄ±t sÃ¼resi alarm
                "min_uptime_percentage": 99.5  # %99.5'den az uptime alarm
            },
            "retention": {
                "keep_daily_reports_days": 30,  # 30 gÃ¼n gÃ¼nlÃ¼k rapor sakla
                "keep_weekly_reports_weeks": 12,  # 12 hafta haftalÄ±k rapor sakla
                "archive_old_reports": True
            }
        }
        
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    default_config.update(user_config)
            except Exception as e:
                print(f"âš ï¸ KonfigÃ¼rasyon dosyasÄ± yÃ¼klenemedi: {e}")
                print("ğŸ”§ VarsayÄ±lan konfigÃ¼rasyon kullanÄ±lÄ±yor")
        
        return default_config
    
    def _ensure_directories(self):
        """Gerekli dizinleri oluÅŸtur"""
        directories = [
            self.reports_dir,
            self.archive_dir,
            self.logs_dir,
            self.reports_dir / "daily",
            self.reports_dir / "weekly",
            self.reports_dir / "hourly",
            self.reports_dir / "html"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
    
    def start_scheduled_testing(self):
        """
        ZamanlanmÄ±ÅŸ testlerin Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ±nÄ± baÅŸlat.
        
        Bu fonksiyon, konfigÃ¼rasyona gÃ¶re testleri otomatik olarak zamanlar.
        """
        print("\nâ° ZAMANLANMIÅ TEST SÄ°STEMÄ° BAÅLATILIYOR")
        print("-" * 50)
        
        # GÃ¼nlÃ¼k tam testler
        if self.config["test_schedule"]["daily_full_tests"]:
            daily_time = self.config["test_schedule"]["daily_full_tests"]
            schedule.every().day.at(daily_time).do(self.run_full_test_suite)
            print(f"ğŸ“… GÃ¼nlÃ¼k tam testler zamanlandÄ±: Her gÃ¼n {daily_time}")
        
        # Saatlik saÄŸlÄ±k kontrolleri
        if self.config["test_schedule"]["hourly_health_checks"]:
            schedule.every().hour.do(self.run_health_check_suite)
            print(f"ğŸ¥ Saatlik saÄŸlÄ±k kontrolleri zamanlandÄ±: Her saat")
        
        # HaftalÄ±k performans testleri
        if self.config["test_schedule"]["weekly_performance_tests"]:
            weekly_schedule = self.config["test_schedule"]["weekly_performance_tests"]
            day, time_str = weekly_schedule.split(" ")
            
            schedule_func = getattr(schedule.every(), day.lower())
            schedule_func.at(time_str).do(self.run_performance_test_suite)
            print(f"ğŸ“Š HaftalÄ±k performans testleri zamanlandÄ±: {weekly_schedule}")
        
        # SÃ¼rekli izleme
        if self.config["test_schedule"]["continuous_monitoring"]:
            schedule.every(15).minutes.do(self.run_monitoring_check)
            print(f"ğŸ” SÃ¼rekli izleme zamanlandÄ±: Her 15 dakikada")
        
        self.is_scheduled = True
        self.current_schedule = {
            "daily_full_tests": self.config["test_schedule"]["daily_full_tests"],
            "hourly_health_checks": self.config["test_schedule"]["hourly_health_checks"],
            "weekly_performance_tests": self.config["test_schedule"]["weekly_performance_tests"],
            "continuous_monitoring": self.config["test_schedule"]["continuous_monitoring"]
        }
        
        print("\nâœ… ZamanlanmÄ±ÅŸ testler aktif!")
        print("ğŸ”„ Testler otomatik olarak Ã§alÄ±ÅŸacak")
        
        return True
    
    def run_scheduled_tests_loop(self):
        """
        ZamanlanmÄ±ÅŸ testlerin ana dÃ¶ngÃ¼sÃ¼.
        
        Bu fonksiyon, schedule kÃ¼tÃ¼phanesi ile zamanlanmÄ±ÅŸ testleri sÃ¼rekli kontrol eder.
        """
        if not self.is_scheduled:
            print("âš ï¸ Ã–nce zamanlanmÄ±ÅŸ testleri baÅŸlatÄ±n: start_scheduled_testing()")
            return
        
        print("\nğŸ”„ ZAMANLANMIÅ TEST DÃ–NGÃœSÃœ BAÅLADI")
        print("=" * 50)
        print("âŒš Testler zamanlarÄ±na gÃ¶re otomatik Ã§alÄ±ÅŸacak")
        print("â¹ï¸ Durdurmak iÃ§in Ctrl+C kullanÄ±n")
        print("=" * 50)
        
        try:
            while True:
                # ZamanlanmÄ±ÅŸ testleri kontrol et ve Ã§alÄ±ÅŸtÄ±r
                schedule.run_pending()
                
                # 60 saniye bekle
                time.sleep(60)
                
                # Her saatte bir durum raporu
                current_time = datetime.now()
                if current_time.minute == 0:
                    self._print_scheduler_status()
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ZamanlanmÄ±ÅŸ testler durduruldu")
            print("ğŸ“Š Son rapor oluÅŸturuluyor...")
            self._generate_scheduler_summary()
    
    def run_full_test_suite(self):
        """
        Tam test paketini Ã§alÄ±ÅŸtÄ±r.
        
        Bu fonksiyon, tÃ¼m test tÃ¼rlerini sÄ±rasÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±r ve kapsamlÄ± rapor oluÅŸturur.
        """
        print("\nğŸ§ª TAM TEST PAKETÄ° Ã‡ALIÅILIYOR")
        print("=" * 50)
        print("ğŸ“… BaÅŸlama zamanÄ±:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        
        test_start_time = time.time()
        
        # Test sonuÃ§larÄ±nÄ± toplamak iÃ§in
        full_test_results = {
            "test_suite": "full_test_suite",
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "total_duration_ms": 0,
            "test_types": {},
            "overall_status": "running",
            "summary": {}
        }
        
        # 1. Unit testleri Ã§alÄ±ÅŸtÄ±r
        if self.config["test_types"]["unit_tests"]:
            print("\nğŸ”¬ Unit testleri Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
            unit_results = self._run_test_type("unit")
            full_test_results["test_types"]["unit_tests"] = unit_results
        
        # 2. Integration testleri Ã§alÄ±ÅŸtÄ±r
        if self.config["test_types"]["integration_tests"]:
            print("\nğŸ”— Integration testleri Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
            integration_results = self._run_test_type("integration")
            full_test_results["test_types"]["integration_tests"] = integration_results
        
        # 3. End-to-end testleri Ã§alÄ±ÅŸtÄ±r
        if self.config["test_types"]["e2e_tests"]:
            print("\nğŸ¬ End-to-end testleri Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
            e2e_results = self._run_test_type("e2e")
            full_test_results["test_types"]["e2e_tests"] = e2e_results
        
        # 4. Fault tolerance testleri Ã§alÄ±ÅŸtÄ±r
        if self.config["test_types"]["fault_tolerance_tests"]:
            print("\nğŸ›¡ï¸ Fault tolerance testleri Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
            fault_results = self._run_test_type("fault_tolerance")
            full_test_results["test_types"]["fault_tolerance_tests"] = fault_results
        
        # 5. Performance testleri Ã§alÄ±ÅŸtÄ±r
        if self.config["test_types"]["performance_tests"]:
            print("\nâš¡ Performance testleri Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
            performance_results = self._run_test_type("performance")
            full_test_results["test_types"]["performance_tests"] = performance_results
        
        # Test sÃ¼resini hesapla
        test_duration = time.time() - test_start_time
        full_test_results["end_time"] = datetime.now().isoformat()
        full_test_results["total_duration_ms"] = test_duration * 1000
        
        # Genel durumu belirle
        overall_status = self._determine_overall_status(full_test_results["test_types"])
        full_test_results["overall_status"] = overall_status
        
        # Ã–zet bilgileri oluÅŸtur
        full_test_results["summary"] = self._generate_test_summary(full_test_results["test_types"])
        
        # Rapor oluÅŸtur
        report_file = self._generate_full_test_report(full_test_results)
        
        print(f"\nğŸ“Š TAM TEST PAKETÄ° TAMAMLANDI")
        print("=" * 50) 
        print(f"â±ï¸ Toplam sÃ¼re: {test_duration:.2f} saniye")
        print(f"ğŸ¯ Genel durum: {overall_status.upper()}")
        print(f"ğŸ“„ Rapor dosyasÄ±: {report_file}")
        print("=" * 50)
        
        # Alarm kontrolÃ¼
        self._check_test_alarms(full_test_results)
        
        return full_test_results
    
    def run_health_check_suite(self):
        """
        SaÄŸlÄ±k kontrolÃ¼ test paketini Ã§alÄ±ÅŸtÄ±r.
        
        Bu fonksiyon, sistemin temel saÄŸlÄ±ÄŸÄ±nÄ± hÄ±zlÄ±ca kontrol eder.
        """
        print("\nğŸ¥ SAÄLIK KONTROLÃœ PAKETÄ° Ã‡ALIÅILIYOR")
        print("-" * 40)
        
        health_start_time = time.time()
        
        health_results = {
            "test_suite": "health_check_suite",
            "start_time": datetime.now().isoformat(),
            "services_checked": {},
            "overall_health": "unknown"
        }
        
        # Servis saÄŸlÄ±k kontrolleri (mock)
        services = [
            "image_processing_service",
            "nlu_service", 
            "style_profile_service",
            "combination_engine_service",
            "recommendation_engine_service",
            "orchestrator_service",
            "feedback_loop_service"
        ]
        
        healthy_services = 0
        total_services = len(services)
        
        for service in services:
            print(f"   ğŸ” {service} kontrol ediliyor...")
            
            # Mock health check (gerÃ§ekte HTTP request olacak)
            is_healthy = self._mock_service_health_check(service)
            response_time = 150 + (hash(service) % 200)  # Mock response time
            
            health_results["services_checked"][service] = {
                "healthy": is_healthy,
                "response_time_ms": response_time,
                "status": "healthy" if is_healthy else "unhealthy"
            }
            
            if is_healthy:
                healthy_services += 1
                print(f"      âœ… SaÄŸlÄ±klÄ± ({response_time}ms)")
            else:
                print(f"      âŒ SaÄŸlÄ±ksÄ±z")
        
        # Genel saÄŸlÄ±k durumunu belirle
        health_percentage = (healthy_services / total_services) * 100
        
        if health_percentage >= 95:
            overall_health = "excellent"
        elif health_percentage >= 80:
            overall_health = "good"
        elif health_percentage >= 60:
            overall_health = "warning"
        else:
            overall_health = "critical"
        
        health_results["overall_health"] = overall_health
        health_results["healthy_services"] = healthy_services
        health_results["total_services"] = total_services
        health_results["health_percentage"] = health_percentage
        health_results["end_time"] = datetime.now().isoformat()
        health_results["duration_ms"] = (time.time() - health_start_time) * 1000
        
        # SaÄŸlÄ±k raporu oluÅŸtur
        health_report_file = self._generate_health_report(health_results)
        
        print(f"\nğŸ“Š SAÄLIK KONTROLÃœ TAMAMLANDI")
        print("-" * 40)
        print(f"â±ï¸ SÃ¼re: {health_results['duration_ms']:.2f}ms")
        print(f"ğŸ¯ Genel saÄŸlÄ±k: {overall_health.upper()}")
        print(f"ğŸ“ˆ SaÄŸlÄ±k oranÄ±: %{health_percentage:.1f} ({healthy_services}/{total_services})")
        print(f"ğŸ“„ Rapor: {health_report_file}")
        
        # SaÄŸlÄ±k alarmlarÄ±
        if health_percentage < 90:
            print(f"ğŸš¨ ALARM: Sistem saÄŸlÄ±ÄŸÄ± dÃ¼ÅŸÃ¼k! %{health_percentage:.1f}")
        
        return health_results
    
    def run_performance_test_suite(self):
        """
        Performans test paketini Ã§alÄ±ÅŸtÄ±r.
        
        Bu fonksiyon, sistemin performans metriklerini detaylÄ± olarak test eder.
        """
        print("\nâš¡ PERFORMANS TEST PAKETÄ° Ã‡ALIÅILIYOR")
        print("-" * 40)
        
        perf_start_time = time.time()
        
        performance_results = {
            "test_suite": "performance_test_suite",
            "start_time": datetime.now().isoformat(),
            "performance_metrics": {},
            "benchmarks": {}
        }
        
        # 1. Response time benchmarks
        print("   ğŸ“Š YanÄ±t sÃ¼resi benchmarklarÄ±...")
        response_benchmarks = self._run_response_time_benchmarks()
        performance_results["benchmarks"]["response_times"] = response_benchmarks
        
        # 2. Throughput tests
        print("   ğŸš€ Throughput testleri...")
        throughput_results = self._run_throughput_tests()
        performance_results["benchmarks"]["throughput"] = throughput_results
        
        # 3. Load tests
        print("   ğŸ‹ï¸ YÃ¼k testleri...")
        load_results = self._run_load_tests()
        performance_results["benchmarks"]["load_handling"] = load_results
        
        # 4. Resource usage
        print("   ğŸ“ˆ Kaynak kullanÄ±mÄ±...")
        resource_results = self._monitor_resource_usage()
        performance_results["benchmarks"]["resource_usage"] = resource_results
        
        # Performans skorunu hesapla
        performance_score = self._calculate_performance_score(performance_results["benchmarks"])
        performance_results["overall_score"] = performance_score
        performance_results["performance_grade"] = self._grade_performance(performance_score)
        
        performance_results["end_time"] = datetime.now().isoformat()
        performance_results["duration_ms"] = (time.time() - perf_start_time) * 1000
        
        # Performans raporu oluÅŸtur
        perf_report_file = self._generate_performance_report(performance_results)
        
        print(f"\nğŸ“Š PERFORMANS TESTLERÄ° TAMAMLANDI")
        print("-" * 40)
        print(f"â±ï¸ SÃ¼re: {performance_results['duration_ms']:.2f}ms")
        print(f"ğŸ† Performans skoru: {performance_score:.1f}/100")
        print(f"ğŸ¯ Performans notu: {performance_results['performance_grade']}")
        print(f"ğŸ“„ Rapor: {perf_report_file}")
        
        return performance_results
    
    def run_monitoring_check(self):
        """
        KÄ±sa monitoring kontrolÃ¼ Ã§alÄ±ÅŸtÄ±r.
        
        Bu fonksiyon, sistemin anlÄ±k durumunu hÄ±zlÄ±ca kontrol eder.
        """
        print(f"\nğŸ” MONITORING KONTROLÃœ - {datetime.now().strftime('%H:%M:%S')}")
        
        monitoring_results = {
            "timestamp": datetime.now().isoformat(),
            "quick_checks": {}
        }
        
        # HÄ±zlÄ± sistem kontrolleri
        checks = {
            "system_uptime": self._check_system_uptime(),
            "service_connectivity": self._check_service_connectivity(), 
            "response_times": self._check_quick_response_times(),
            "error_rates": self._check_error_rates()
        }
        
        monitoring_results["quick_checks"] = checks
        
        # Kritik durumlarÄ± kontrol et
        critical_issues = []
        for check_name, check_result in checks.items():
            if check_result.get("status") == "critical":
                critical_issues.append(f"{check_name}: {check_result.get('message', 'Unknown issue')}")
        
        if critical_issues:
            print("ğŸš¨ KRÄ°TÄ°K SORUNLAR TESPÄ°T EDÄ°LDÄ°:")
            for issue in critical_issues:
                print(f"   âŒ {issue}")
        else:
            print("   âœ… TÃ¼m kontrollerden geÃ§ti")
        
        # Monitoring log'a kaydet
        self._log_monitoring_result(monitoring_results)
        
        return monitoring_results
    
    def _run_test_type(self, test_type: str) -> Dict:
        """Belirli bir test tÃ¼rÃ¼nÃ¼ Ã§alÄ±ÅŸtÄ±r"""
        try:
            # Mock test execution (gerÃ§ekte pytest subprocess Ã§alÄ±ÅŸtÄ±rÄ±lacak)
            test_start = time.time()
            
            # Test tÃ¼rÃ¼ne gÃ¶re simÃ¼le edilmiÅŸ sonuÃ§lar
            mock_results = {
                "unit": {
                    "total_tests": 45,
                    "passed": 43,
                    "failed": 2,
                    "skipped": 0,
                    "duration_ms": 2340
                },
                "integration": {
                    "total_tests": 12, 
                    "passed": 11,
                    "failed": 1,
                    "skipped": 0,
                    "duration_ms": 8700
                },
                "e2e": {
                    "total_tests": 8,
                    "passed": 8,
                    "failed": 0,
                    "skipped": 0,
                    "duration_ms": 15600
                },
                "fault_tolerance": {
                    "total_tests": 15,
                    "passed": 14,
                    "failed": 1,
                    "skipped": 0,
                    "duration_ms": 5200
                },
                "performance": {
                    "total_tests": 10,
                    "passed": 9,
                    "failed": 1,
                    "skipped": 0,
                    "duration_ms": 25800
                }
            }
            
            result = mock_results.get(test_type, {
                "total_tests": 5,
                "passed": 4,
                "failed": 1,
                "skipped": 0,
                "duration_ms": 3000
            })
            
            # BaÅŸarÄ± oranÄ±nÄ± hesapla
            if result["total_tests"] > 0:
                success_rate = (result["passed"] / result["total_tests"]) * 100
            else:
                success_rate = 0
            
            result["success_rate"] = success_rate
            result["status"] = "passed" if success_rate >= 90 else "failed" if success_rate < 70 else "warning"
            
            actual_duration = (time.time() - test_start) * 1000
            print(f"      âœ… {test_type} testleri tamamlandÄ± ({actual_duration:.0f}ms)")
            print(f"         ğŸ“Š {result['passed']}/{result['total_tests']} test baÅŸarÄ±lÄ± (%{success_rate:.1f})")
            
            return result
            
        except Exception as e:
            print(f"      âŒ {test_type} testleri baÅŸarÄ±sÄ±z: {str(e)}")
            return {
                "status": "error",
                "error": str(e),
                "total_tests": 0,
                "passed": 0,
                "failed": 0,
                "success_rate": 0
            }
    
    def _mock_service_health_check(self, service_name: str) -> bool:
        """Mock servis saÄŸlÄ±k kontrolÃ¼"""
        # Hash'e dayalÄ± deterministic ama realistic sonuÃ§lar
        health_score = hash(service_name + str(int(time.time() / 3600))) % 100
        return health_score > 15  # %85 uptime simÃ¼lasyonu
    
    def _determine_overall_status(self, test_results: Dict) -> str:
        """Test sonuÃ§larÄ±ndan genel durumu belirle"""
        if not test_results:
            return "no_tests"
        
        statuses = [result.get("status", "unknown") for result in test_results.values()]
        
        if all(status == "passed" for status in statuses):
            return "passed"
        elif any(status == "error" for status in statuses):
            return "error"
        elif any(status == "failed" for status in statuses):
            return "failed"
        else:
            return "warning"
    
    def _generate_test_summary(self, test_results: Dict) -> Dict:
        """Test sonuÃ§larÄ±ndan Ã¶zet bilgiler oluÅŸtur"""
        summary = {
            "total_test_suites": len(test_results),
            "total_tests": 0,
            "total_passed": 0,
            "total_failed": 0,
            "total_skipped": 0,
            "overall_success_rate": 0
        }
        
        for result in test_results.values():
            summary["total_tests"] += result.get("total_tests", 0)
            summary["total_passed"] += result.get("passed", 0)
            summary["total_failed"] += result.get("failed", 0)
            summary["total_skipped"] += result.get("skipped", 0)
        
        if summary["total_tests"] > 0:
            summary["overall_success_rate"] = (summary["total_passed"] / summary["total_tests"]) * 100
        
        return summary
    
    def _generate_full_test_report(self, test_results: Dict) -> str:
        """Tam test raporu oluÅŸtur"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_filename = f"full_test_report_{timestamp}.json"
        report_filepath = self.reports_dir / "daily" / report_filename
        
        # JSON raporu kaydet
        with open(report_filepath, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False)
        
        # HTML raporu da oluÅŸtur
        if self.config["reporting"]["html_reports"]:
            html_filename = f"full_test_report_{timestamp}.html"
            html_filepath = self.reports_dir / "html" / html_filename
            self._generate_html_report(test_results, html_filepath)
        
        return str(report_filepath)
    
    def _generate_health_report(self, health_results: Dict) -> str:
        """SaÄŸlÄ±k kontrolÃ¼ raporu oluÅŸtur"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_filename = f"health_check_{timestamp}.json"
        report_filepath = self.reports_dir / "hourly" / report_filename
        
        with open(report_filepath, 'w', encoding='utf-8') as f:
            json.dump(health_results, f, indent=2, ensure_ascii=False)
        
        return str(report_filepath)
    
    def _generate_performance_report(self, perf_results: Dict) -> str:
        """Performans test raporu oluÅŸtur"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_filename = f"performance_report_{timestamp}.json"
        report_filepath = self.reports_dir / "weekly" / report_filename
        
        with open(report_filepath, 'w', encoding='utf-8') as f:
            json.dump(perf_results, f, indent=2, ensure_ascii=False)
        
        return str(report_filepath)
    
    def _generate_html_report(self, test_results: Dict, output_file: Path):
        """HTML formatÄ±nda rapor oluÅŸtur"""
        html_content = f"""
<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aura AI Test Raporu</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; color: #333; }}
        .status-passed {{ color: #28a745; }}
        .status-failed {{ color: #dc3545; }}
        .status-warning {{ color: #ffc107; }}
        .metric {{ display: inline-block; margin: 10px; padding: 15px; background: #f8f9fa; border-radius: 5px; text-align: center; }}
        .test-suite {{ margin: 20px 0; padding: 15px; border-left: 4px solid #007bff; background: #f8f9fa; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background: #343a40; color: white; }}
        .footer {{ text-align: center; margin-top: 30px; color: #666; font-size: 12px; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ§ª Aura AI Sistem Test Raporu</h1>
            <p>Test Tarihi: {test_results.get('start_time', 'N/A')}</p>
            <p class="status-{test_results.get('overall_status', 'unknown').lower()}">
                Genel Durum: {test_results.get('overall_status', 'UNKNOWN').upper()}
            </p>
        </div>
        
        <div class="metrics">
            <div class="metric">
                <h3>â±ï¸ Toplam SÃ¼re</h3>
                <p>{test_results.get('total_duration_ms', 0)/1000:.2f} saniye</p>
            </div>
            <div class="metric">
                <h3>ğŸ“Š Test TÃ¼rleri</h3>
                <p>{len(test_results.get('test_types', {}))}</p>
            </div>
            <div class="metric">
                <h3>âœ… BaÅŸarÄ± OranÄ±</h3>
                <p>%{test_results.get('summary', {}).get('overall_success_rate', 0):.1f}</p>
            </div>
        </div>
        
        <h2>ğŸ“‹ Test SonuÃ§larÄ± DetayÄ±</h2>
        <table>
            <thead>
                <tr>
                    <th>Test TÃ¼rÃ¼</th>
                    <th>Durum</th>
                    <th>Toplam</th>
                    <th>BaÅŸarÄ±lÄ±</th>
                    <th>BaÅŸarÄ±sÄ±z</th>
                    <th>BaÅŸarÄ± OranÄ±</th>
                </tr>
            </thead>
            <tbody>
"""
        
        # Test tÃ¼rleri tablosunu doldur
        for test_type, results in test_results.get('test_types', {}).items():
            status_class = f"status-{results.get('status', 'unknown').lower()}"
            html_content += f"""
                <tr>
                    <td>{test_type.replace('_', ' ').title()}</td>
                    <td class="{status_class}">{results.get('status', 'UNKNOWN').upper()}</td>
                    <td>{results.get('total_tests', 0)}</td>
                    <td>{results.get('passed', 0)}</td>
                    <td>{results.get('failed', 0)}</td>
                    <td>%{results.get('success_rate', 0):.1f}</td>
                </tr>
"""
        
        html_content += f"""
            </tbody>
        </table>
        
        <div class="footer">
            <p>Bu rapor Aura AI Test Otomasyon Sistemi tarafÄ±ndan otomatik olarak oluÅŸturulmuÅŸtur.</p>
            <p>Rapor oluÅŸturulma zamanÄ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
"""
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def _check_test_alarms(self, test_results: Dict):
        """Test sonuÃ§larÄ±nda alarm durumlarÄ±nÄ± kontrol et"""
        summary = test_results.get('summary', {})
        overall_success_rate = summary.get('overall_success_rate', 0)
        
        # BaÅŸarÄ± oranÄ± alarm kontrolÃ¼
        if overall_success_rate < (100 - self.config['thresholds']['max_failure_rate']):
            print(f"ğŸš¨ ALARM: Test baÅŸarÄ± oranÄ± Ã§ok dÃ¼ÅŸÃ¼k! %{overall_success_rate:.1f}")
            self._send_alarm_notification("Test BaÅŸarÄ± OranÄ± DÃ¼ÅŸÃ¼k", f"BaÅŸarÄ± oranÄ±: %{overall_success_rate:.1f}")
        
        # Duration alarm kontrolÃ¼  
        total_duration_ms = test_results.get('total_duration_ms', 0)
        if total_duration_ms > (self.config['thresholds']['max_response_time_ms'] * 10):  # Test suite 10x threshold
            print(f"ğŸš¨ ALARM: Test sÃ¼resi Ã§ok uzun! {total_duration_ms/1000:.2f} saniye")
            self._send_alarm_notification("Test SÃ¼resi Uzun", f"SÃ¼re: {total_duration_ms/1000:.2f}s")
    
    def _send_alarm_notification(self, title: str, message: str):
        """Alarm bildirimi gÃ¶nder"""
        # Demo iÃ§in sadece log'a yaz
        alarm_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        alarm_log = f"[{alarm_time}] ALARM: {title} - {message}\n"
        
        alarm_log_file = self.logs_dir / "alarms.log"
        with open(alarm_log_file, 'a', encoding='utf-8') as f:
            f.write(alarm_log)
    
    def _print_scheduler_status(self):
        """Scheduler durumunu yazdÄ±r"""
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"\nğŸ“… Scheduler Durumu ({current_time}):")
        print(f"   ğŸ”„ Aktif zamanlamalar: {len(schedule.jobs)}")
        
        # Bir sonraki Ã§alÄ±ÅŸacak job'larÄ± gÃ¶ster
        next_jobs = sorted(schedule.jobs, key=lambda x: x.next_run)[:3]
        print(f"   â° Sonraki testler:")
        for job in next_jobs:
            print(f"      - {job.next_run.strftime('%H:%M')} - {job.job_func.__name__}")
    
    def _generate_scheduler_summary(self):
        """Scheduler Ã¶zet raporu oluÅŸtur"""
        summary = {
            "scheduler_session": {
                "start_time": datetime.now().isoformat(),
                "total_scheduled_jobs": len(schedule.jobs),
                "active_schedules": self.current_schedule
            }
        }
        
        summary_file = self.reports_dir / f"scheduler_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ Scheduler Ã¶zet raporu: {summary_file}")
    
    # Mock performance test functions
    def _run_response_time_benchmarks(self) -> Dict:
        return {"avg_response_ms": 245, "p95_response_ms": 890, "status": "good"}
    
    def _run_throughput_tests(self) -> Dict:
        return {"requests_per_second": 67, "concurrent_users": 25, "status": "good"}
    
    def _run_load_tests(self) -> Dict:
        return {"max_load_handled": 150, "breaking_point_rps": 200, "status": "good"}
    
    def _monitor_resource_usage(self) -> Dict:
        return {"avg_cpu_percent": 45, "avg_memory_mb": 512, "status": "good"}
    
    def _calculate_performance_score(self, benchmarks: Dict) -> float:
        return 78.5  # Mock performance score
    
    def _grade_performance(self, score: float) -> str:
        if score >= 90:
            return "A"
        elif score >= 80:
            return "B"
        elif score >= 70:
            return "C"
        elif score >= 60:
            return "D"
        else:
            return "F"
    
    # Mock monitoring functions
    def _check_system_uptime(self) -> Dict:
        return {"status": "healthy", "uptime_hours": 72, "message": "System running normally"}
    
    def _check_service_connectivity(self) -> Dict:
        return {"status": "healthy", "services_online": 7, "services_total": 8, "message": "Most services online"}
    
    def _check_quick_response_times(self) -> Dict:
        return {"status": "healthy", "avg_response_ms": 230, "message": "Response times normal"}
    
    def _check_error_rates(self) -> Dict:
        return {"status": "healthy", "error_rate_percent": 0.5, "message": "Error rates low"}
    
    def _log_monitoring_result(self, result: Dict):
        """Monitoring sonucunu log'a kaydet"""
        log_file = self.logs_dir / "monitoring.log"
        log_entry = f"[{result['timestamp']}] MONITORING: {json.dumps(result['quick_checks'])}\n"
        
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry)

def main():
    """Ana test otomasyon sistemi"""
    print("ğŸ¤– AURA AI TEST OTOMASYON SÄ°STEMÄ°")
    print("=" * 60)
    
    # Test otomasyon nesnesini oluÅŸtur
    automation = AuraTestAutomation()
    
    # Komut satÄ±rÄ± argÃ¼manlarÄ±nÄ± kontrol et
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "schedule":
            # ZamanlanmÄ±ÅŸ testleri baÅŸlat
            automation.start_scheduled_testing()
            automation.run_scheduled_tests_loop()
            
        elif command == "full":
            # Tam test paketini Ã§alÄ±ÅŸtÄ±r
            results = automation.run_full_test_suite()
            return 0 if results['overall_status'] in ['passed', 'warning'] else 1
            
        elif command == "health":
            # SaÄŸlÄ±k kontrolÃ¼ Ã§alÄ±ÅŸtÄ±r
            results = automation.run_health_check_suite()
            return 0 if results['overall_health'] in ['excellent', 'good'] else 1
            
        elif command == "performance":
            # Performans testleri Ã§alÄ±ÅŸtÄ±r
            results = automation.run_performance_test_suite()
            return 0 if results['overall_score'] >= 70 else 1
            
        elif command == "monitor":
            # Monitoring kontrolÃ¼ Ã§alÄ±ÅŸtÄ±r
            results = automation.run_monitoring_check()
            return 0
            
        else:
            print(f"âŒ Bilinmeyen komut: {command}")
            print("ğŸ’¡ KullanÄ±labilir komutlar: schedule, full, health, performance, monitor")
            return 1
    else:
        # ArgÃ¼man verilmemiÅŸse interactive mode
        print("ğŸ® Ä°NTERAKTÄ°F MOD")
        print("-" * 40)
        print("1. ZamanlanmÄ±ÅŸ testleri baÅŸlat (schedule)")
        print("2. Tam test paketi Ã§alÄ±ÅŸtÄ±r (full)")
        print("3. SaÄŸlÄ±k kontrolÃ¼ (health)")
        print("4. Performans testleri (performance)")
        print("5. Monitoring kontrolÃ¼ (monitor)")
        print("-" * 40)
        
        choice = input("SeÃ§iminizi yapÄ±n (1-5): ").strip()
        
        if choice == "1":
            automation.start_scheduled_testing()
            automation.run_scheduled_tests_loop()
        elif choice == "2":
            results = automation.run_full_test_suite()
            return 0 if results['overall_status'] in ['passed', 'warning'] else 1
        elif choice == "3":
            results = automation.run_health_check_suite()
            return 0 if results['overall_health'] in ['excellent', 'good'] else 1
        elif choice == "4":
            results = automation.run_performance_test_suite()
            return 0 if results['overall_score'] >= 70 else 1
        elif choice == "5":
            results = automation.run_monitoring_check()
            return 0
        else:
            print("âŒ GeÃ§ersiz seÃ§im")
            return 1
    
    return 0

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Test otomasyonu kullanÄ±cÄ± tarafÄ±ndan durduruldu")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ Test otomasyonunda beklenmeyen hata: {str(e)}")
        sys.exit(1)
